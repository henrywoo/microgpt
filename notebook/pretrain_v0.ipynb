{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from microgpt.model import MicroGPTConfig, MicroGPT\n",
    "from hiq.vis import print_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "\n",
    "# model\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# Output directory for training checkpoints\n",
    "out_dir = \"out\"  # Default output directory\n",
    "\n",
    "# Sampling configuration\n",
    "# This out_dir is also used by sample.py for loading trained models\n",
    "eval_interval = 250  # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "log_interval = 10  # don't print too too often\n",
    "\n",
    "# we expect to overfit on this small dataset, so only save when val improves\n",
    "always_save_checkpoint = False\n",
    "dataset = \"shakespeare_char\"\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 64\n",
    "block_size = 256  # context of up to 256 previous characters\n",
    "\n",
    "# microGPT setup\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
    "max_iters = 5000\n",
    "lr_decay_iters = 5000  # make equal to max_iters usually\n",
    "min_lr = 1e-4  # learning_rate / 10 usually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d27f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e973704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 16,384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "# single GPU setup\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on matmul\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader - data_dir will be set in main() after config is loaded\n",
    "def get_batch(split, data_dir):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f7d0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 29.94M\n",
      "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">üå≥ MicroGPT&lt;all params:30036864&gt;</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleDict</span><span style=\"color: #808000; text-decoration-color: #808000\">(transformer)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wte)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[50304,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wpe)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[256,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleList</span><span style=\"color: #808000; text-decoration-color: #808000\">(h)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ </span>üí† <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Block</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(0-5)&lt;ü¶ú:1770240x6&gt;</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚î£‚îÅ‚îÅ </span>üí† <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">LayerNorm</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(ln_1,ln_2)&lt;ü¶ú:384x2&gt;|</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">CausalSelfAttention</span><span style=\"color: #808000; text-decoration-color: #808000\">(attn)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îÉ   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_attn)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1152,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îÉ   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">MLP</span><span style=\"color: #808000; text-decoration-color: #808000\">(mlp)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_fc)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1536,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,1536]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îÇ   ‚îî‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">LayerNorm</span><span style=\"color: #808000; text-decoration-color: #808000\">(ln_f)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îî‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(lm_head)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[50304,384]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33müå≥ MicroGPT<all params:30036864>\u001b[0m\n",
       "\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mModuleDict\u001b[0m\u001b[33m(transformer)\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wte)|\u001b[0m\u001b[94mweight[50304,384]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wpe)|\u001b[0m\u001b[94mweight[256,384]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mModuleList\u001b[0m\u001b[33m(h)\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0müí† \u001b]8;id=919686;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mBlock\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=152329;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(0-5)<ü¶ú:1770240x6>\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚î£‚îÅ‚îÅ \u001b[0müí† \u001b]8;id=129072;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mLayerNorm\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=916368;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(ln_1,ln_2)<ü¶ú:384x2>|\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=895704;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[94mweight[384]\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mCausalSelfAttention\u001b[0m\u001b[33m(attn)\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îÉ   \u001b[0m\u001b[38;5;37m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_attn)|\u001b[0m\u001b[94mweight[1152,384]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îÉ   \u001b[0m\u001b[38;5;37m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,384]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mMLP\u001b[0m\u001b[33m(mlp)\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_fc)|\u001b[0m\u001b[94mweight[1536,384]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,1536]\u001b[0m\n",
       "\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0m\u001b[32mLayerNorm\u001b[0m\u001b[33m(ln_f)|\u001b[0m\u001b[94mweight[384]\u001b[0m\n",
       "\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(lm_head)|\u001b[0m\u001b[94mweight[50304,384]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init these up here\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init - always initialize a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) # start with model_args from command line\n",
    "gptconf = MicroGPTConfig(**model_args)\n",
    "model = MicroGPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "print_model(model)\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e1e0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_dir):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, data_dir)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "269476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99c8261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global iter_num, best_val_loss, model, optimizer\n",
    "    \n",
    "    # training loop\n",
    "    # data_dir will be set here after config is loaded\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    \n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    \n",
    "    # update model_args with vocab_size if found\n",
    "    if meta_vocab_size is not None:\n",
    "        model_args['vocab_size'] = meta_vocab_size\n",
    "        # recreate the model with the correct vocab_size\n",
    "        gptconf = MicroGPTConfig(**model_args)\n",
    "        model = MicroGPT(gptconf)\n",
    "        model.to(device)\n",
    "        # recompile if needed\n",
    "        if compile:\n",
    "            print(\"recompiling the model with updated vocab_size...\")\n",
    "            unoptimized_model = model\n",
    "            model = torch.compile(model)\n",
    "        # reconfigure optimizer\n",
    "        optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "    \n",
    "    # Print the final model structure after all potential modifications\n",
    "    from hiq.vis import print_model\n",
    "    print_model(model)\n",
    "    \n",
    "    X, Y = get_batch('train', data_dir) # fetch the very first batch\n",
    "    t0 = time.time()\n",
    "    local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "    raw_model = model # no DDP wrapper needed\n",
    "    running_mfu = -1.0\n",
    "    while True:\n",
    "\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0 and master_process:\n",
    "            losses = estimate_loss(data_dir)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'model_args': model_args,\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': config,\n",
    "                    }\n",
    "                    print(f\"saving checkpoint to {out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "        if iter_num == 0 and eval_only:\n",
    "            break\n",
    "\n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train', data_dir)\n",
    "            # backward pass, with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0 and master_process:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "\n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1bbeda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to: /home/wukong/git.repo/pypi/microgpt/notebook/data/shakespeare_char\n",
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python -m microgpt.prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54aea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n",
      "recompiling the model with updated vocab_size...\n",
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">üå≥ OptimizedModule&lt;all params:10745088&gt;</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îî‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">MicroGPT</span><span style=\"color: #808000; text-decoration-color: #808000\">(_orig_mod)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleDict</span><span style=\"color: #808000; text-decoration-color: #808000\">(transformer)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wte)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[65,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wpe)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[256,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îú‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleList</span><span style=\"color: #808000; text-decoration-color: #808000\">(h)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ </span>üí† <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Block</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(0-5)&lt;ü¶ú:1770240x6&gt;</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚î£‚îÅ‚îÅ </span>üí† <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">LayerNorm</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(ln_1,ln_2)&lt;ü¶ú:384x2&gt;|</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">CausalSelfAttention</span><span style=\"color: #808000; text-decoration-color: #808000\">(attn)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îÉ   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_attn)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1152,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îÉ   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">MLP</span><span style=\"color: #808000; text-decoration-color: #808000\">(mlp)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚î£‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_fc)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1536,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îÇ       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">‚îó‚îÅ‚îÅ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,1536]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îÇ   ‚îî‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">LayerNorm</span><span style=\"color: #808000; text-decoration-color: #808000\">(ln_f)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ‚îî‚îÄ‚îÄ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(lm_head)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[65,384]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33müå≥ OptimizedModule<all params:10745088>\u001b[0m\n",
       "\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0m\u001b[32mMicroGPT\u001b[0m\u001b[33m(_orig_mod)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mModuleDict\u001b[0m\u001b[33m(transformer)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wte)|\u001b[0m\u001b[94mweight[65,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wpe)|\u001b[0m\u001b[94mweight[256,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îú‚îÄ‚îÄ \u001b[0m\u001b[32mModuleList\u001b[0m\u001b[33m(h)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0müí† \u001b]8;id=138392;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mBlock\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=730366;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(0-5)<ü¶ú:1770240x6>\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚î£‚îÅ‚îÅ \u001b[0müí† \u001b]8;id=444550;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mLayerNorm\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=206309;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(ln_1,ln_2)<ü¶ú:384x2>|\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=827623;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[94mweight[384]\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mCausalSelfAttention\u001b[0m\u001b[33m(attn)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îÉ   \u001b[0m\u001b[38;5;37m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_attn)|\u001b[0m\u001b[94mweight[1152,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îÉ   \u001b[0m\u001b[38;5;37m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mMLP\u001b[0m\u001b[33m(mlp)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m‚î£‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_fc)|\u001b[0m\u001b[94mweight[1536,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m‚îó‚îÅ‚îÅ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,1536]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îÇ   \u001b[0m\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0m\u001b[32mLayerNorm\u001b[0m\u001b[33m(ln_f)|\u001b[0m\u001b[94mweight[384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m‚îî‚îÄ‚îÄ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(lm_head)|\u001b[0m\u001b[94mweight[65,384]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3128, val loss 4.3080\n",
      "iter 0: loss 4.2885, time 2146.30ms, mfu -100.00%\n",
      "iter 10: loss 3.2350, time 15.21ms, mfu 24.49%\n",
      "iter 20: loss 2.8015, time 15.19ms, mfu 24.50%\n",
      "iter 30: loss 2.6265, time 14.97ms, mfu 24.54%\n",
      "iter 40: loss 2.5540, time 15.31ms, mfu 24.52%\n",
      "iter 50: loss 2.5377, time 15.06ms, mfu 24.54%\n",
      "iter 60: loss 2.5120, time 15.62ms, mfu 24.47%\n",
      "iter 70: loss 2.5035, time 14.83ms, mfu 24.54%\n",
      "iter 80: loss 2.4890, time 14.26ms, mfu 24.70%\n",
      "iter 90: loss 2.4903, time 14.90ms, mfu 24.73%\n",
      "iter 100: loss 2.4622, time 14.69ms, mfu 24.79%\n",
      "iter 110: loss 2.4527, time 15.16ms, mfu 24.77%\n",
      "iter 120: loss 2.4160, time 14.97ms, mfu 24.78%\n",
      "iter 130: loss 2.4440, time 15.39ms, mfu 24.72%\n",
      "iter 140: loss 2.4301, time 15.06ms, mfu 24.73%\n",
      "iter 150: loss 2.3881, time 14.46ms, mfu 24.83%\n",
      "iter 160: loss 2.3564, time 14.73ms, mfu 24.88%\n",
      "iter 170: loss 2.3446, time 15.04ms, mfu 24.87%\n",
      "iter 180: loss 2.3481, time 14.85ms, mfu 24.89%\n",
      "iter 190: loss 2.3067, time 14.37ms, mfu 24.99%\n",
      "iter 200: loss 2.2148, time 14.94ms, mfu 24.99%\n",
      "iter 210: loss 2.1879, time 14.88ms, mfu 24.99%\n",
      "iter 220: loss 2.1636, time 14.68ms, mfu 25.03%\n",
      "iter 230: loss 2.1202, time 14.69ms, mfu 25.06%\n",
      "iter 240: loss 2.0617, time 14.86ms, mfu 25.07%\n",
      "step 250: train loss 1.9841, val loss 2.0724\n",
      "saving checkpoint to out\n",
      "iter 250: loss 2.0143, time 2304.23ms, mfu 22.58%\n",
      "iter 260: loss 2.0134, time 15.27ms, mfu 22.76%\n",
      "iter 270: loss 1.9956, time 14.74ms, mfu 23.01%\n",
      "iter 280: loss 1.9537, time 15.05ms, mfu 23.19%\n",
      "iter 290: loss 1.9329, time 14.69ms, mfu 23.40%\n",
      "iter 300: loss 1.8907, time 15.09ms, mfu 23.53%\n",
      "iter 310: loss 1.8886, time 15.04ms, mfu 23.66%\n",
      "iter 320: loss 1.8839, time 14.99ms, mfu 23.78%\n",
      "iter 330: loss 1.8564, time 14.76ms, mfu 23.93%\n",
      "iter 340: loss 1.8151, time 15.16ms, mfu 23.99%\n",
      "iter 350: loss 1.8064, time 14.54ms, mfu 24.16%\n",
      "iter 360: loss 1.7833, time 15.06ms, mfu 24.21%\n",
      "iter 370: loss 1.7860, time 14.83ms, mfu 24.30%\n",
      "iter 380: loss 1.7871, time 15.17ms, mfu 24.33%\n",
      "iter 390: loss 1.7692, time 14.71ms, mfu 24.43%\n",
      "iter 400: loss 1.7353, time 14.49ms, mfu 24.56%\n",
      "iter 410: loss 1.7385, time 14.89ms, mfu 24.60%\n",
      "iter 420: loss 1.7221, time 15.04ms, mfu 24.62%\n",
      "iter 430: loss 1.6994, time 14.78ms, mfu 24.68%\n",
      "iter 440: loss 1.6683, time 14.74ms, mfu 24.74%\n",
      "iter 450: loss 1.6601, time 15.14ms, mfu 24.73%\n",
      "iter 460: loss 1.6648, time 15.06ms, mfu 24.73%\n",
      "iter 470: loss 1.6292, time 14.79ms, mfu 24.78%\n",
      "iter 480: loss 1.6319, time 14.43ms, mfu 24.88%\n",
      "iter 490: loss 1.6411, time 14.74ms, mfu 24.92%\n",
      "step 500: train loss 1.5351, val loss 1.7106\n",
      "saving checkpoint to out\n",
      "iter 500: loss 1.6246, time 2297.44ms, mfu 22.44%\n",
      "iter 510: loss 1.5816, time 14.44ms, mfu 22.78%\n",
      "iter 520: loss 1.6049, time 14.84ms, mfu 23.01%\n",
      "iter 530: loss 1.6015, time 15.11ms, mfu 23.18%\n",
      "iter 540: loss 1.5393, time 15.06ms, mfu 23.33%\n",
      "iter 550: loss 1.5926, time 14.35ms, mfu 23.60%\n",
      "iter 560: loss 1.5470, time 15.27ms, mfu 23.68%\n",
      "iter 570: loss 1.5745, time 14.76ms, mfu 23.83%\n",
      "iter 580: loss 1.5341, time 14.43ms, mfu 24.03%\n",
      "iter 590: loss 1.5814, time 15.21ms, mfu 24.08%\n",
      "iter 600: loss 1.5226, time 15.08ms, mfu 24.14%\n",
      "iter 610: loss 1.5122, time 14.95ms, mfu 24.22%\n",
      "iter 620: loss 1.5022, time 14.70ms, mfu 24.33%\n",
      "iter 630: loss 1.5066, time 15.14ms, mfu 24.36%\n",
      "iter 640: loss 1.4779, time 14.87ms, mfu 24.43%\n",
      "iter 650: loss 1.5225, time 15.08ms, mfu 24.46%\n",
      "iter 660: loss 1.4983, time 14.80ms, mfu 24.53%\n",
      "iter 670: loss 1.5185, time 14.81ms, mfu 24.59%\n",
      "iter 680: loss 1.4537, time 14.88ms, mfu 24.64%\n",
      "iter 690: loss 1.4733, time 15.16ms, mfu 24.63%\n",
      "iter 700: loss 1.4716, time 14.56ms, mfu 24.73%\n",
      "iter 710: loss 1.4604, time 14.24ms, mfu 24.87%\n",
      "iter 720: loss 1.4529, time 15.05ms, mfu 24.86%\n",
      "iter 730: loss 1.4607, time 14.61ms, mfu 24.93%\n",
      "iter 740: loss 1.4262, time 15.00ms, mfu 24.92%\n",
      "step 750: train loss 1.3593, val loss 1.5768\n",
      "saving checkpoint to out\n",
      "iter 750: loss 1.4340, time 2304.49ms, mfu 22.44%\n",
      "iter 760: loss 1.4426, time 15.02ms, mfu 22.68%\n",
      "iter 770: loss 1.4038, time 14.02ms, mfu 23.07%\n",
      "iter 780: loss 1.4146, time 15.08ms, mfu 23.23%\n",
      "iter 790: loss 1.4219, time 14.59ms, mfu 23.46%\n",
      "iter 800: loss 1.4262, time 15.01ms, mfu 23.60%\n",
      "iter 810: loss 1.4317, time 13.77ms, mfu 23.95%\n",
      "iter 820: loss 1.3939, time 14.88ms, mfu 24.06%\n",
      "iter 830: loss 1.4382, time 14.97ms, mfu 24.14%\n",
      "iter 840: loss 1.4069, time 15.25ms, mfu 24.17%\n",
      "iter 850: loss 1.3857, time 14.22ms, mfu 24.37%\n",
      "iter 860: loss 1.3811, time 15.08ms, mfu 24.41%\n",
      "iter 870: loss 1.4032, time 14.81ms, mfu 24.48%\n",
      "iter 880: loss 1.3615, time 15.30ms, mfu 24.47%\n",
      "iter 890: loss 1.3868, time 14.06ms, mfu 24.67%\n",
      "iter 900: loss 1.3827, time 15.06ms, mfu 24.68%\n",
      "iter 910: loss 1.3942, time 14.68ms, mfu 24.75%\n",
      "iter 920: loss 1.3718, time 15.06ms, mfu 24.75%\n",
      "iter 930: loss 1.3454, time 13.81ms, mfu 24.97%\n",
      "iter 940: loss 1.3353, time 15.17ms, mfu 24.93%\n",
      "iter 950: loss 1.3634, time 14.65ms, mfu 24.98%\n",
      "iter 960: loss 1.3492, time 15.07ms, mfu 24.96%\n",
      "iter 970: loss 1.3579, time 14.01ms, mfu 25.12%\n",
      "iter 980: loss 1.3599, time 14.87ms, mfu 25.11%\n",
      "iter 990: loss 1.3518, time 14.65ms, mfu 25.15%\n",
      "step 1000: train loss 1.2696, val loss 1.5245\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 1.3604, time 2309.21ms, mfu 22.65%\n",
      "iter 1010: loss 1.3300, time 15.01ms, mfu 22.87%\n",
      "iter 1020: loss 1.3429, time 14.22ms, mfu 23.20%\n",
      "iter 1030: loss 1.3703, time 15.22ms, mfu 23.33%\n",
      "iter 1040: loss 1.3321, time 14.05ms, mfu 23.65%\n",
      "iter 1050: loss 1.3256, time 14.83ms, mfu 23.79%\n",
      "iter 1060: loss 1.3592, time 14.61ms, mfu 23.96%\n",
      "iter 1070: loss 1.3442, time 14.99ms, mfu 24.05%\n",
      "iter 1080: loss 1.3239, time 14.38ms, mfu 24.24%\n",
      "iter 1090: loss 1.3166, time 14.90ms, mfu 24.32%\n",
      "iter 1100: loss 1.3025, time 15.04ms, mfu 24.36%\n",
      "iter 1110: loss 1.3176, time 15.12ms, mfu 24.39%\n",
      "iter 1120: loss 1.2987, time 14.36ms, mfu 24.55%\n",
      "iter 1130: loss 1.2972, time 15.22ms, mfu 24.54%\n",
      "iter 1140: loss 1.2984, time 14.78ms, mfu 24.61%\n",
      "iter 1150: loss 1.3028, time 14.93ms, mfu 24.64%\n",
      "iter 1160: loss 1.3008, time 14.84ms, mfu 24.69%\n",
      "iter 1170: loss 1.2851, time 14.64ms, mfu 24.76%\n",
      "iter 1180: loss 1.2838, time 14.81ms, mfu 24.80%\n",
      "iter 1190: loss 1.3026, time 15.19ms, mfu 24.78%\n",
      "iter 1200: loss 1.2822, time 14.90ms, mfu 24.80%\n",
      "iter 1210: loss 1.2602, time 14.62ms, mfu 24.87%\n",
      "iter 1220: loss 1.2584, time 14.79ms, mfu 24.90%\n",
      "iter 1230: loss 1.2658, time 15.32ms, mfu 24.84%\n",
      "iter 1240: loss 1.2878, time 14.80ms, mfu 24.88%\n",
      "step 1250: train loss 1.2042, val loss 1.4957\n",
      "saving checkpoint to out\n",
      "iter 1250: loss 1.2735, time 2315.08ms, mfu 22.41%\n",
      "iter 1260: loss 1.2593, time 15.13ms, mfu 22.63%\n",
      "iter 1270: loss 1.2921, time 14.88ms, mfu 22.87%\n",
      "iter 1280: loss 1.2763, time 15.03ms, mfu 23.06%\n",
      "iter 1290: loss 1.2824, time 14.77ms, mfu 23.28%\n",
      "iter 1300: loss 1.2414, time 14.85ms, mfu 23.46%\n",
      "iter 1310: loss 1.2318, time 14.79ms, mfu 23.63%\n",
      "iter 1320: loss 1.2558, time 15.04ms, mfu 23.75%\n",
      "iter 1330: loss 1.2838, time 14.22ms, mfu 23.99%\n",
      "iter 1340: loss 1.2381, time 15.04ms, mfu 24.07%\n",
      "iter 1350: loss 1.2672, time 15.27ms, mfu 24.10%\n",
      "iter 1360: loss 1.2654, time 15.21ms, mfu 24.14%\n",
      "iter 1370: loss 1.2785, time 14.23ms, mfu 24.35%\n",
      "iter 1380: loss 1.2293, time 15.43ms, mfu 24.33%\n",
      "iter 1390: loss 1.2510, time 14.62ms, mfu 24.44%\n",
      "iter 1400: loss 1.2540, time 15.25ms, mfu 24.44%\n",
      "iter 1410: loss 1.2365, time 15.02ms, mfu 24.48%\n",
      "iter 1420: loss 1.2229, time 15.30ms, mfu 24.47%\n",
      "iter 1430: loss 1.2635, time 14.25ms, mfu 24.63%\n",
      "iter 1440: loss 1.2480, time 15.22ms, mfu 24.62%\n",
      "iter 1450: loss 1.2535, time 15.30ms, mfu 24.59%\n",
      "iter 1460: loss 1.1885, time 15.10ms, mfu 24.60%\n",
      "iter 1470: loss 1.2283, time 14.64ms, mfu 24.69%\n",
      "iter 1480: loss 1.2327, time 14.16ms, mfu 24.85%\n",
      "iter 1490: loss 1.2311, time 15.19ms, mfu 24.82%\n",
      "step 1500: train loss 1.1440, val loss 1.4671\n",
      "saving checkpoint to out\n",
      "iter 1500: loss 1.2063, time 2316.20ms, mfu 22.35%\n",
      "iter 1510: loss 1.2190, time 14.10ms, mfu 22.76%\n",
      "iter 1520: loss 1.2264, time 15.11ms, mfu 22.95%\n",
      "iter 1530: loss 1.2001, time 14.43ms, mfu 23.24%\n",
      "iter 1540: loss 1.1864, time 15.02ms, mfu 23.39%\n",
      "iter 1550: loss 1.2030, time 14.13ms, mfu 23.69%\n",
      "iter 1560: loss 1.2033, time 15.46ms, mfu 23.73%\n",
      "iter 1570: loss 1.2106, time 14.49ms, mfu 23.93%\n",
      "iter 1580: loss 1.2147, time 14.87ms, mfu 24.04%\n",
      "iter 1590: loss 1.2128, time 14.65ms, mfu 24.18%\n",
      "iter 1600: loss 1.1922, time 15.10ms, mfu 24.23%\n",
      "iter 1610: loss 1.2124, time 14.42ms, mfu 24.39%\n",
      "iter 1620: loss 1.2060, time 14.90ms, mfu 24.45%\n",
      "iter 1630: loss 1.1971, time 14.88ms, mfu 24.51%\n",
      "iter 1640: loss 1.1839, time 15.08ms, mfu 24.53%\n",
      "iter 1650: loss 1.1979, time 14.81ms, mfu 24.59%\n",
      "iter 1660: loss 1.1969, time 15.34ms, mfu 24.56%\n",
      "iter 1670: loss 1.2165, time 14.94ms, mfu 24.60%\n",
      "iter 1680: loss 1.1807, time 15.14ms, mfu 24.60%\n",
      "iter 1690: loss 1.1982, time 15.16ms, mfu 24.60%\n",
      "iter 1700: loss 1.1590, time 14.81ms, mfu 24.66%\n",
      "iter 1710: loss 1.1975, time 14.76ms, mfu 24.71%\n",
      "iter 1720: loss 1.1897, time 15.20ms, mfu 24.69%\n",
      "iter 1730: loss 1.1556, time 14.53ms, mfu 24.79%\n",
      "iter 1740: loss 1.2109, time 14.46ms, mfu 24.89%\n",
      "step 1750: train loss 1.0986, val loss 1.4555\n",
      "saving checkpoint to out\n",
      "iter 1750: loss 1.1666, time 2331.28ms, mfu 22.42%\n",
      "iter 1760: loss 1.2360, time 15.26ms, mfu 22.62%\n",
      "iter 1770: loss 1.1915, time 14.45ms, mfu 22.93%\n",
      "iter 1780: loss 1.1914, time 15.65ms, mfu 23.02%\n",
      "iter 1790: loss 1.1631, time 15.32ms, mfu 23.15%\n",
      "iter 1800: loss 1.1610, time 14.50ms, mfu 23.41%\n",
      "iter 1810: loss 1.1467, time 15.26ms, mfu 23.51%\n",
      "iter 1820: loss 1.1471, time 14.87ms, mfu 23.66%\n",
      "iter 1830: loss 1.1487, time 14.87ms, mfu 23.80%\n",
      "iter 1840: loss 1.1643, time 15.60ms, mfu 23.81%\n",
      "iter 1850: loss 1.1898, time 15.02ms, mfu 23.91%\n",
      "iter 1860: loss 1.1485, time 14.42ms, mfu 24.10%\n",
      "iter 1870: loss 1.1862, time 14.92ms, mfu 24.19%\n",
      "iter 1880: loss 1.1369, time 14.81ms, mfu 24.29%\n",
      "iter 1890: loss 1.1391, time 15.79ms, mfu 24.22%\n",
      "iter 1900: loss 1.1310, time 15.44ms, mfu 24.21%\n",
      "iter 1910: loss 1.1610, time 14.99ms, mfu 24.28%\n",
      "iter 1920: loss 1.1572, time 15.17ms, mfu 24.30%\n",
      "iter 1930: loss 1.1763, time 20.61ms, mfu 23.68%\n",
      "iter 1940: loss 1.1584, time 15.12ms, mfu 23.78%\n",
      "iter 1950: loss 1.1374, time 15.17ms, mfu 23.86%\n",
      "iter 1960: loss 1.1354, time 15.14ms, mfu 23.93%\n",
      "iter 1970: loss 1.1597, time 20.82ms, mfu 23.33%\n",
      "iter 1980: loss 1.1568, time 14.33ms, mfu 23.60%\n",
      "iter 1990: loss 1.1208, time 15.21ms, mfu 23.69%\n",
      "step 2000: train loss 1.0489, val loss 1.4693\n",
      "iter 2000: loss 1.1538, time 2216.17ms, mfu 21.34%\n",
      "iter 2010: loss 1.1308, time 14.62ms, mfu 21.75%\n",
      "iter 2020: loss 1.1595, time 15.23ms, mfu 22.02%\n",
      "iter 2030: loss 1.1348, time 15.24ms, mfu 22.26%\n",
      "iter 2040: loss 1.1411, time 14.45ms, mfu 22.62%\n",
      "iter 2050: loss 1.1329, time 15.18ms, mfu 22.81%\n",
      "iter 2060: loss 1.1183, time 14.51ms, mfu 23.10%\n",
      "iter 2070: loss 1.1142, time 15.06ms, mfu 23.26%\n",
      "iter 2080: loss 1.1215, time 14.72ms, mfu 23.47%\n",
      "iter 2090: loss 1.1035, time 14.76ms, mfu 23.64%\n",
      "iter 2100: loss 1.0920, time 16.16ms, mfu 23.59%\n",
      "iter 2110: loss 1.1312, time 15.77ms, mfu 23.59%\n",
      "iter 2120: loss 1.1111, time 14.42ms, mfu 23.82%\n",
      "iter 2130: loss 1.1045, time 15.21ms, mfu 23.88%\n",
      "iter 2140: loss 1.0892, time 14.62ms, mfu 24.04%\n",
      "iter 2150: loss 1.1254, time 15.08ms, mfu 24.11%\n",
      "iter 2160: loss 1.1110, time 15.07ms, mfu 24.17%\n",
      "iter 2170: loss 1.1471, time 14.52ms, mfu 24.32%\n",
      "iter 2180: loss 1.0929, time 15.06ms, mfu 24.36%\n",
      "iter 2190: loss 1.1270, time 16.08ms, mfu 24.25%\n",
      "iter 2200: loss 1.1043, time 15.02ms, mfu 24.30%\n",
      "iter 2210: loss 1.0720, time 14.89ms, mfu 24.37%\n",
      "iter 2220: loss 1.0942, time 14.97ms, mfu 24.43%\n",
      "iter 2230: loss 1.1029, time 15.08ms, mfu 24.45%\n",
      "iter 2240: loss 1.0963, time 14.59ms, mfu 24.56%\n",
      "step 2250: train loss 1.0032, val loss 1.4680\n",
      "iter 2250: loss 1.1174, time 2222.48ms, mfu 22.12%\n",
      "iter 2260: loss 1.0793, time 14.47ms, mfu 22.49%\n",
      "iter 2270: loss 1.0925, time 15.25ms, mfu 22.68%\n",
      "iter 2280: loss 1.1034, time 15.55ms, mfu 22.81%\n",
      "iter 2290: loss 1.0827, time 15.06ms, mfu 23.00%\n",
      "iter 2300: loss 1.0692, time 15.39ms, mfu 23.12%\n",
      "iter 2310: loss 1.0945, time 15.26ms, mfu 23.25%\n",
      "iter 2320: loss 1.0515, time 14.42ms, mfu 23.51%\n",
      "iter 2330: loss 1.1186, time 15.36ms, mfu 23.59%\n",
      "iter 2340: loss 1.0986, time 14.67ms, mfu 23.77%\n",
      "iter 2350: loss 1.0893, time 15.57ms, mfu 23.79%\n",
      "iter 2360: loss 1.1193, time 15.58ms, mfu 23.80%\n",
      "iter 2370: loss 1.0995, time 14.63ms, mfu 23.97%\n",
      "iter 2380: loss 1.0601, time 15.04ms, mfu 24.05%\n",
      "iter 2390: loss 1.0822, time 14.66ms, mfu 24.18%\n",
      "iter 2400: loss 1.1119, time 14.68ms, mfu 24.30%\n",
      "iter 2410: loss 1.0710, time 15.78ms, mfu 24.23%\n",
      "iter 2420: loss 1.0700, time 14.67ms, mfu 24.35%\n",
      "iter 2430: loss 1.0977, time 15.62ms, mfu 24.30%\n",
      "iter 2440: loss 1.0995, time 15.02ms, mfu 24.35%\n",
      "iter 2450: loss 1.0901, time 14.69ms, mfu 24.45%\n",
      "iter 2460: loss 1.0605, time 15.12ms, mfu 24.47%\n",
      "iter 2470: loss 1.0335, time 15.19ms, mfu 24.48%\n",
      "iter 2480: loss 1.0230, time 15.05ms, mfu 24.51%\n",
      "iter 2490: loss 1.0791, time 14.63ms, mfu 24.60%\n",
      "step 2500: train loss 0.9550, val loss 1.4899\n",
      "iter 2500: loss 1.0512, time 2213.42ms, mfu 22.16%\n",
      "iter 2510: loss 1.0745, time 15.33ms, mfu 22.37%\n",
      "iter 2520: loss 1.1079, time 14.40ms, mfu 22.72%\n",
      "iter 2530: loss 1.0702, time 15.21ms, mfu 22.90%\n",
      "iter 2540: loss 1.0443, time 15.28ms, mfu 23.05%\n",
      "iter 2550: loss 1.0491, time 14.63ms, mfu 23.29%\n",
      "iter 2560: loss 1.0733, time 15.17ms, mfu 23.42%\n",
      "iter 2570: loss 1.0553, time 14.60ms, mfu 23.63%\n",
      "iter 2580: loss 1.0486, time 15.18ms, mfu 23.72%\n",
      "iter 2590: loss 1.0321, time 15.16ms, mfu 23.81%\n",
      "iter 2600: loss 1.0668, time 15.29ms, mfu 23.86%\n",
      "iter 2610: loss 1.0709, time 16.24ms, mfu 23.77%\n",
      "iter 2620: loss 1.0738, time 14.49ms, mfu 23.97%\n",
      "iter 2630: loss 1.0278, time 15.53ms, mfu 23.97%\n",
      "iter 2640: loss 1.0420, time 14.49ms, mfu 24.14%\n",
      "iter 2650: loss 1.0771, time 20.29ms, mfu 23.56%\n",
      "iter 2660: loss 1.0384, time 15.74ms, mfu 23.58%\n",
      "iter 2670: loss 1.0375, time 15.11ms, mfu 23.68%\n",
      "iter 2680: loss 1.0388, time 14.59ms, mfu 23.87%\n",
      "iter 2690: loss 1.0304, time 16.25ms, mfu 23.78%\n",
      "iter 2700: loss 1.0456, time 15.13ms, mfu 23.86%\n",
      "iter 2710: loss 1.0306, time 15.20ms, mfu 23.93%\n",
      "iter 2720: loss 1.0338, time 15.07ms, mfu 24.01%\n",
      "iter 2730: loss 1.0121, time 14.63ms, mfu 24.15%\n",
      "iter 2740: loss 1.0158, time 15.44ms, mfu 24.15%\n",
      "step 2750: train loss 0.9070, val loss 1.5114\n",
      "iter 2750: loss 1.0444, time 2204.08ms, mfu 21.75%\n",
      "iter 2760: loss 1.0322, time 15.13ms, mfu 22.04%\n",
      "iter 2770: loss 1.0243, time 15.17ms, mfu 22.29%\n",
      "iter 2780: loss 1.0197, time 14.74ms, mfu 22.59%\n",
      "iter 2790: loss 1.0054, time 15.22ms, mfu 22.78%\n",
      "iter 2800: loss 1.0271, time 14.61ms, mfu 23.05%\n",
      "iter 2810: loss 1.0270, time 15.14ms, mfu 23.21%\n",
      "iter 2820: loss 1.0303, time 15.19ms, mfu 23.34%\n",
      "iter 2830: loss 1.0352, time 14.78ms, mfu 23.53%\n",
      "iter 2840: loss 1.0204, time 14.86ms, mfu 23.68%\n",
      "iter 2850: loss 1.0200, time 14.99ms, mfu 23.80%\n",
      "iter 2860: loss 1.0333, time 14.72ms, mfu 23.95%\n",
      "iter 2870: loss 1.0202, time 14.81ms, mfu 24.07%\n",
      "iter 2880: loss 0.9887, time 15.29ms, mfu 24.10%\n",
      "iter 2890: loss 1.0014, time 14.84ms, mfu 24.20%\n",
      "iter 2900: loss 1.0241, time 15.70ms, mfu 24.16%\n",
      "iter 2910: loss 1.0092, time 14.86ms, mfu 24.25%\n",
      "iter 2920: loss 1.0061, time 16.08ms, mfu 24.14%\n",
      "iter 2930: loss 0.9827, time 14.71ms, mfu 24.26%\n",
      "iter 2940: loss 0.9783, time 15.35ms, mfu 24.26%\n",
      "iter 2950: loss 0.9827, time 15.09ms, mfu 24.30%\n",
      "iter 2960: loss 1.0021, time 14.67ms, mfu 24.41%\n",
      "iter 2970: loss 0.9888, time 14.66ms, mfu 24.51%\n",
      "iter 2980: loss 0.9769, time 14.44ms, mfu 24.64%\n",
      "iter 2990: loss 1.0152, time 15.56ms, mfu 24.57%\n",
      "step 3000: train loss 0.8603, val loss 1.5430\n",
      "iter 3000: loss 0.9711, time 2208.61ms, mfu 22.13%\n",
      "iter 3010: loss 0.9831, time 14.63ms, mfu 22.47%\n",
      "iter 3020: loss 1.0083, time 15.35ms, mfu 22.65%\n",
      "iter 3030: loss 0.9919, time 15.74ms, mfu 22.75%\n",
      "iter 3040: loss 0.9503, time 14.68ms, mfu 23.01%\n",
      "iter 3050: loss 0.9633, time 14.84ms, mfu 23.22%\n",
      "iter 3060: loss 0.9972, time 14.66ms, mfu 23.44%\n",
      "iter 3070: loss 0.9745, time 15.46ms, mfu 23.51%\n",
      "iter 3080: loss 0.9780, time 15.30ms, mfu 23.59%\n",
      "iter 3090: loss 0.9740, time 14.38ms, mfu 23.83%\n",
      "iter 3100: loss 0.9867, time 15.26ms, mfu 23.89%\n",
      "iter 3110: loss 0.9702, time 14.36ms, mfu 24.09%\n",
      "iter 3120: loss 0.9787, time 15.30ms, mfu 24.12%\n",
      "iter 3130: loss 0.9792, time 14.88ms, mfu 24.21%\n",
      "iter 3140: loss 0.9726, time 14.89ms, mfu 24.29%\n",
      "iter 3150: loss 0.9516, time 14.59ms, mfu 24.42%\n",
      "iter 3160: loss 0.9852, time 15.03ms, mfu 24.45%\n",
      "iter 3170: loss 0.9824, time 15.26ms, mfu 24.45%\n",
      "iter 3180: loss 0.9321, time 14.83ms, mfu 24.52%\n",
      "iter 3190: loss 0.9675, time 15.10ms, mfu 24.53%\n",
      "iter 3200: loss 0.9651, time 15.18ms, mfu 24.54%\n",
      "iter 3210: loss 0.9532, time 14.43ms, mfu 24.66%\n",
      "iter 3220: loss 0.9795, time 15.41ms, mfu 24.62%\n",
      "iter 3230: loss 0.9693, time 14.38ms, mfu 24.75%\n",
      "iter 3240: loss 0.9587, time 15.15ms, mfu 24.73%\n",
      "step 3250: train loss 0.8102, val loss 1.5635\n",
      "iter 3250: loss 0.9465, time 2231.25ms, mfu 22.27%\n",
      "iter 3260: loss 0.9330, time 15.42ms, mfu 22.46%\n",
      "iter 3270: loss 0.9531, time 14.79ms, mfu 22.74%\n",
      "iter 3280: loss 0.9212, time 15.31ms, mfu 22.90%\n",
      "iter 3290: loss 0.9501, time 14.71ms, mfu 23.14%\n",
      "iter 3300: loss 0.9563, time 14.73ms, mfu 23.36%\n",
      "iter 3310: loss 0.9716, time 15.35ms, mfu 23.45%\n",
      "iter 3320: loss 0.9447, time 14.69ms, mfu 23.64%\n",
      "iter 3330: loss 0.9602, time 18.24ms, mfu 23.32%\n",
      "iter 3340: loss 0.9698, time 14.92ms, mfu 23.48%\n",
      "iter 3350: loss 0.9688, time 15.41ms, mfu 23.55%\n",
      "iter 3360: loss 0.9470, time 15.11ms, mfu 23.66%\n",
      "iter 3370: loss 0.9635, time 20.27ms, mfu 23.14%\n",
      "iter 3380: loss 0.9389, time 15.32ms, mfu 23.26%\n",
      "iter 3390: loss 0.9232, time 14.56ms, mfu 23.49%\n",
      "iter 3400: loss 0.9357, time 15.13ms, mfu 23.60%\n",
      "iter 3410: loss 0.9375, time 18.60ms, mfu 23.25%\n",
      "iter 3420: loss 0.9081, time 14.83ms, mfu 23.43%\n",
      "iter 3430: loss 0.9398, time 15.04ms, mfu 23.57%\n",
      "iter 3440: loss 0.9113, time 14.83ms, mfu 23.72%\n",
      "iter 3450: loss 0.9562, time 15.00ms, mfu 23.84%\n",
      "iter 3460: loss 0.9568, time 15.24ms, mfu 23.90%\n",
      "iter 3470: loss 0.9143, time 14.26ms, mfu 24.12%\n",
      "iter 3480: loss 0.9177, time 15.14ms, mfu 24.17%\n",
      "iter 3490: loss 0.9108, time 14.92ms, mfu 24.25%\n",
      "step 3500: train loss 0.7707, val loss 1.5831\n",
      "iter 3500: loss 0.9133, time 2194.67ms, mfu 21.84%\n",
      "iter 3510: loss 0.9234, time 14.56ms, mfu 22.22%\n",
      "iter 3520: loss 0.8820, time 14.59ms, mfu 22.55%\n",
      "iter 3530: loss 0.9057, time 15.53ms, mfu 22.69%\n",
      "iter 3540: loss 0.9265, time 14.62ms, mfu 22.97%\n",
      "iter 3550: loss 0.9289, time 14.45ms, mfu 23.25%\n",
      "iter 3560: loss 0.9447, time 15.25ms, mfu 23.37%\n",
      "iter 3570: loss 0.9038, time 14.34ms, mfu 23.63%\n",
      "iter 3580: loss 0.9217, time 15.29ms, mfu 23.71%\n",
      "iter 3590: loss 0.9301, time 14.65ms, mfu 23.88%\n",
      "iter 3600: loss 0.9415, time 15.10ms, mfu 23.96%\n",
      "iter 3610: loss 0.9076, time 14.62ms, mfu 24.11%\n",
      "iter 3620: loss 0.9170, time 15.15ms, mfu 24.16%\n",
      "iter 3630: loss 0.9115, time 15.65ms, mfu 24.13%\n",
      "iter 3640: loss 0.9217, time 15.14ms, mfu 24.18%\n",
      "iter 3650: loss 0.9162, time 15.65ms, mfu 24.14%\n",
      "iter 3660: loss 0.8722, time 15.53ms, mfu 24.13%\n",
      "iter 3670: loss 0.9129, time 15.11ms, mfu 24.18%\n",
      "iter 3680: loss 0.9158, time 14.91ms, mfu 24.26%\n",
      "iter 3690: loss 0.8982, time 15.29ms, mfu 24.27%\n",
      "iter 3700: loss 0.8991, time 15.17ms, mfu 24.30%\n",
      "iter 3710: loss 0.9043, time 15.07ms, mfu 24.34%\n",
      "iter 3720: loss 0.8889, time 15.37ms, mfu 24.33%\n",
      "iter 3730: loss 0.9080, time 14.53ms, mfu 24.46%\n",
      "iter 3740: loss 0.8912, time 15.25ms, mfu 24.46%\n",
      "step 3750: train loss 0.7321, val loss 1.6135\n",
      "iter 3750: loss 0.9044, time 2203.81ms, mfu 22.03%\n",
      "iter 3760: loss 0.8841, time 14.88ms, mfu 22.33%\n",
      "iter 3770: loss 0.9170, time 15.50ms, mfu 22.51%\n",
      "iter 3780: loss 0.8690, time 15.41ms, mfu 22.67%\n",
      "iter 3790: loss 0.8864, time 14.48ms, mfu 22.98%\n",
      "iter 3800: loss 0.8903, time 14.65ms, mfu 23.23%\n",
      "iter 3810: loss 0.8848, time 15.09ms, mfu 23.37%\n",
      "iter 3820: loss 0.8779, time 14.57ms, mfu 23.59%\n",
      "iter 3830: loss 0.8860, time 15.50ms, mfu 23.64%\n",
      "iter 3840: loss 0.9144, time 15.75ms, mfu 23.64%\n",
      "iter 3850: loss 0.8870, time 14.69ms, mfu 23.81%\n",
      "iter 3860: loss 0.8837, time 15.32ms, mfu 23.86%\n",
      "iter 3870: loss 0.8741, time 14.90ms, mfu 23.98%\n",
      "iter 3880: loss 0.8791, time 14.48ms, mfu 24.15%\n",
      "iter 3890: loss 0.8793, time 15.07ms, mfu 24.21%\n",
      "iter 3900: loss 0.8809, time 14.93ms, mfu 24.29%\n",
      "iter 3910: loss 0.8917, time 15.34ms, mfu 24.29%\n",
      "iter 3920: loss 0.8877, time 15.17ms, mfu 24.31%\n",
      "iter 3930: loss 0.8661, time 14.76ms, mfu 24.41%\n",
      "iter 3940: loss 0.8782, time 15.20ms, mfu 24.42%\n",
      "iter 3950: loss 0.8738, time 14.59ms, mfu 24.53%\n",
      "iter 3960: loss 0.8817, time 14.91ms, mfu 24.58%\n",
      "iter 3970: loss 0.8916, time 15.06ms, mfu 24.59%\n",
      "iter 3980: loss 0.8835, time 14.66ms, mfu 24.68%\n",
      "iter 3990: loss 0.8586, time 15.07ms, mfu 24.68%\n",
      "step 4000: train loss 0.6963, val loss 1.6425\n",
      "iter 4000: loss 0.8626, time 2197.43ms, mfu 22.23%\n",
      "iter 4010: loss 0.8805, time 15.06ms, mfu 22.48%\n",
      "iter 4020: loss 0.8668, time 15.06ms, mfu 22.71%\n",
      "iter 4030: loss 0.8965, time 15.29ms, mfu 22.87%\n",
      "iter 4040: loss 0.8794, time 15.20ms, mfu 23.04%\n",
      "iter 4050: loss 0.8736, time 15.22ms, mfu 23.18%\n",
      "iter 4060: loss 0.8621, time 15.59ms, mfu 23.25%\n",
      "iter 4070: loss 0.8547, time 14.26ms, mfu 23.54%\n",
      "iter 4080: loss 0.8406, time 15.47ms, mfu 23.60%\n",
      "iter 4090: loss 0.8749, time 18.32ms, mfu 23.27%\n",
      "iter 4100: loss 0.8562, time 14.86ms, mfu 23.45%\n",
      "iter 4110: loss 0.8481, time 15.43ms, mfu 23.52%\n",
      "iter 4120: loss 0.8354, time 15.37ms, mfu 23.59%\n",
      "iter 4130: loss 0.8654, time 19.40ms, mfu 23.15%\n",
      "iter 4140: loss 0.8564, time 15.49ms, mfu 23.24%\n",
      "iter 4150: loss 0.8634, time 14.80ms, mfu 23.44%\n",
      "iter 4160: loss 0.8559, time 15.41ms, mfu 23.51%\n",
      "iter 4170: loss 0.8488, time 16.91ms, mfu 23.36%\n",
      "iter 4180: loss 0.8554, time 14.46ms, mfu 23.60%\n",
      "iter 4190: loss 0.8434, time 15.95ms, mfu 23.58%\n",
      "iter 4200: loss 0.8627, time 15.69ms, mfu 23.60%\n",
      "iter 4210: loss 0.8625, time 15.00ms, mfu 23.72%\n",
      "iter 4220: loss 0.8756, time 15.25ms, mfu 23.79%\n",
      "iter 4230: loss 0.8590, time 15.24ms, mfu 23.86%\n",
      "iter 4240: loss 0.8327, time 14.78ms, mfu 23.99%\n",
      "step 4250: train loss 0.6687, val loss 1.6531\n",
      "iter 4250: loss 0.8636, time 2201.92ms, mfu 21.61%\n",
      "iter 4260: loss 0.8297, time 15.12ms, mfu 21.92%\n",
      "iter 4270: loss 0.8369, time 15.06ms, mfu 22.20%\n",
      "iter 4280: loss 0.8780, time 15.30ms, mfu 22.41%\n",
      "iter 4290: loss 0.8572, time 14.88ms, mfu 22.68%\n",
      "iter 4300: loss 0.8463, time 14.96ms, mfu 22.90%\n",
      "iter 4310: loss 0.8327, time 15.42ms, mfu 23.03%\n",
      "iter 4320: loss 0.8613, time 15.14ms, mfu 23.18%\n",
      "iter 4330: loss 0.8421, time 15.39ms, mfu 23.29%\n",
      "iter 4340: loss 0.8609, time 15.11ms, mfu 23.43%\n",
      "iter 4350: loss 0.8471, time 15.32ms, mfu 23.52%\n",
      "iter 4360: loss 0.8555, time 15.05ms, mfu 23.64%\n",
      "iter 4370: loss 0.8656, time 15.28ms, mfu 23.72%\n",
      "iter 4380: loss 0.8356, time 14.64ms, mfu 23.89%\n",
      "iter 4390: loss 0.8572, time 14.67ms, mfu 24.04%\n",
      "iter 4400: loss 0.8279, time 15.26ms, mfu 24.08%\n",
      "iter 4410: loss 0.8264, time 14.67ms, mfu 24.21%\n",
      "iter 4420: loss 0.8328, time 15.22ms, mfu 24.24%\n",
      "iter 4430: loss 0.8248, time 14.90ms, mfu 24.31%\n",
      "iter 4440: loss 0.8228, time 14.46ms, mfu 24.46%\n",
      "iter 4450: loss 0.8313, time 15.23ms, mfu 24.46%\n",
      "iter 4460: loss 0.8184, time 18.55ms, mfu 24.02%\n",
      "iter 4470: loss 0.8418, time 14.60ms, mfu 24.17%\n",
      "iter 4480: loss 0.8397, time 15.30ms, mfu 24.19%\n",
      "iter 4490: loss 0.8033, time 14.49ms, mfu 24.34%\n",
      "step 4500: train loss 0.6440, val loss 1.6704\n",
      "iter 4500: loss 0.8445, time 2201.71ms, mfu 21.93%\n",
      "iter 4510: loss 0.8537, time 14.74ms, mfu 22.26%\n",
      "iter 4520: loss 0.8273, time 14.96ms, mfu 22.53%\n",
      "iter 4530: loss 0.8210, time 15.50ms, mfu 22.68%\n",
      "iter 4540: loss 0.8221, time 14.70ms, mfu 22.94%\n",
      "iter 4550: loss 0.8291, time 15.13ms, mfu 23.11%\n",
      "iter 4560: loss 0.8118, time 15.02ms, mfu 23.28%\n",
      "iter 4570: loss 0.8181, time 14.77ms, mfu 23.48%\n",
      "iter 4580: loss 0.8207, time 15.17ms, mfu 23.58%\n",
      "iter 4590: loss 0.8600, time 14.93ms, mfu 23.72%\n",
      "iter 4600: loss 0.8286, time 15.32ms, mfu 23.78%\n",
      "iter 4610: loss 0.8289, time 14.91ms, mfu 23.90%\n",
      "iter 4620: loss 0.8359, time 14.82ms, mfu 24.03%\n",
      "iter 4630: loss 0.8117, time 15.09ms, mfu 24.09%\n",
      "iter 4640: loss 0.7949, time 14.65ms, mfu 24.23%\n",
      "iter 4650: loss 0.8131, time 15.42ms, mfu 24.22%\n",
      "iter 4660: loss 0.8042, time 15.28ms, mfu 24.24%\n",
      "iter 4670: loss 0.8188, time 16.23ms, mfu 24.11%\n",
      "iter 4680: loss 0.8346, time 14.95ms, mfu 24.19%\n",
      "iter 4690: loss 0.8113, time 15.37ms, mfu 24.20%\n",
      "iter 4700: loss 0.8278, time 14.27ms, mfu 24.39%\n",
      "iter 4710: loss 0.8375, time 15.32ms, mfu 24.38%\n",
      "iter 4720: loss 0.8257, time 15.44ms, mfu 24.36%\n",
      "iter 4730: loss 0.8180, time 14.51ms, mfu 24.49%\n",
      "iter 4740: loss 0.8255, time 15.46ms, mfu 24.45%\n",
      "step 4750: train loss 0.6238, val loss 1.6866\n",
      "iter 4750: loss 0.8226, time 2216.26ms, mfu 22.02%\n",
      "iter 4760: loss 0.8176, time 15.00ms, mfu 22.30%\n",
      "iter 4770: loss 0.8025, time 17.22ms, mfu 22.24%\n",
      "iter 4780: loss 0.8177, time 15.22ms, mfu 22.46%\n",
      "iter 4790: loss 0.8250, time 14.81ms, mfu 22.73%\n",
      "iter 4800: loss 0.8037, time 15.30ms, mfu 22.89%\n",
      "iter 4810: loss 0.7971, time 16.13ms, mfu 22.91%\n",
      "iter 4820: loss 0.8206, time 14.92ms, mfu 23.12%\n",
      "iter 4830: loss 0.8390, time 15.58ms, mfu 23.20%\n",
      "iter 4840: loss 0.8025, time 14.59ms, mfu 23.43%\n",
      "iter 4850: loss 0.8198, time 15.07ms, mfu 23.56%\n",
      "iter 4860: loss 0.8068, time 15.34ms, mfu 23.63%\n",
      "iter 4870: loss 0.8166, time 14.63ms, mfu 23.82%\n",
      "iter 4880: loss 0.7967, time 15.33ms, mfu 23.87%\n",
      "iter 4890: loss 0.8209, time 15.26ms, mfu 23.92%\n",
      "iter 4900: loss 0.8226, time 15.99ms, mfu 23.86%\n",
      "iter 4910: loss 0.8178, time 15.40ms, mfu 23.89%\n",
      "iter 4920: loss 0.8086, time 14.62ms, mfu 24.05%\n",
      "iter 4930: loss 0.8086, time 14.60ms, mfu 24.20%\n",
      "iter 4940: loss 0.8043, time 15.38ms, mfu 24.20%\n",
      "iter 4950: loss 0.8240, time 14.77ms, mfu 24.30%\n",
      "iter 4960: loss 0.7723, time 15.49ms, mfu 24.28%\n",
      "iter 4970: loss 0.8105, time 15.37ms, mfu 24.27%\n",
      "iter 4980: loss 0.8231, time 14.42ms, mfu 24.43%\n",
      "iter 4990: loss 0.7927, time 15.22ms, mfu 24.44%\n",
      "step 5000: train loss 0.6091, val loss 1.7015\n",
      "iter 5000: loss 0.8255, time 2233.62ms, mfu 22.01%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f7f51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /home/wukong/git.repo/pypi/microgpt/microgpt/pretrain/config.py\n",
      "Using out_dir: out\n",
      "Loading checkpoint from: out/ckpt.pt\n",
      "number of parameters: 10.65M\n",
      "Using data directory meta.pkl from: data/shakespeare_char/meta.pkl\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "ANTIGONUS:\n",
      "O, well, if they could have been earned to my calamation\n",
      "Which in his chief? We throw, ever crave themselves,\n",
      "Would be some fit to be tastended.\n",
      "\n",
      "CAMILLO:\n",
      "Even in this:\n",
      "Give me them to me no honourable. A little courtier, courage:\n",
      "And in some word will may not passion with this odd\n",
      "May keep the other Capitol with this increase,\n",
      "We still move for us.\n",
      "\n",
      "POLIXENES:\n",
      "A son; if he were not male of all\n",
      "For her by our red-full assident, what a customary\n",
      "Have he straight no more stood of care,\n",
      "---------------\n",
      "\n",
      "Menenius, we are too fools and master and\n",
      "severeign perfected.\n",
      "\n",
      "CORIOLANUS:\n",
      "What is the city, whom\n",
      "consentents?\n",
      "\n",
      "MENENIUS:\n",
      "I cannot lack to thee ever.\n",
      "\n",
      "CORIOLANUS:\n",
      "Approceed of the evil stifles of the rest, and\n",
      "shall continues to the good duke of Rome\n",
      "and honour and to the court with Corioli buckle country.\n",
      "\n",
      "CORIOLANUS:\n",
      "O come, is this I see\n",
      "My change not with the every office of the noble earth\n",
      "Hath motion like other to counterfeit of kindred the gentleman\n",
      "That I may be contented: for the good \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python -m microgpt.sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaibook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
