{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from microgpt.model import MicroGPTConfig, MicroGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e4b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from /home/wukong/git.repo/pypi/microgpt/notebook/config.py\n",
      "Dataset: shakespeare_char\n",
      "Out dir: out\n",
      "Batch size: 64\n",
      "Block size: 256\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare_char'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration overrides from config.py\"\"\"\n",
    "    try:\n",
    "        # Get the directory where this script is located\n",
    "        # Handle both script and notebook environments\n",
    "        try:\n",
    "            # For script environment\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # For notebook environment - use current working directory\n",
    "            script_dir = os.getcwd()\n",
    "            # If we're in a notebook, try to find the pretrain directory\n",
    "            if 'pretrain' not in script_dir:\n",
    "                # Look for pretrain directory in current path or parent directories\n",
    "                current_dir = script_dir\n",
    "                for _ in range(3):  # Look up to 3 levels up\n",
    "                    if os.path.exists(os.path.join(current_dir, 'pretrain')):\n",
    "                        script_dir = os.path.join(current_dir, 'pretrain')\n",
    "                        break\n",
    "                    parent_dir = os.path.dirname(current_dir)\n",
    "                    if parent_dir == current_dir:  # Reached root\n",
    "                        break\n",
    "                    current_dir = parent_dir\n",
    "        \n",
    "        config_path = os.path.join(script_dir, 'config.py')\n",
    "        \n",
    "        # Read and execute the config file\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_content = f.read()\n",
    "        \n",
    "        # Execute the config content in the current globals\n",
    "        exec(config_content, globals())\n",
    "        \n",
    "        print(f\"Configuration loaded from {config_path}\")\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        print(f\"Out dir: {out_dir}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Block size: {block_size}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"config.py not found at {config_path}, using default configuration\")\n",
    "        pass  # config.py is optional\n",
    "\n",
    "# Load configuration immediately when module is imported\n",
    "load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d27f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e973704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 16,384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "# single GPU setup\n",
    "master_process = True\n",
    "seed_offset = 0\n",
    "ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on matmul\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader - data_dir will be set in main() after config is loaded\n",
    "def get_batch(split, data_dir):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7d0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 29.94M\n",
      "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init these up here\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init - always initialize a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50304, dropout=dropout) # start with model_args from command line\n",
    "gptconf = MicroGPTConfig(**model_args)\n",
    "model = MicroGPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0f87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">🌳 OptimizedModule&lt;all params:30036864&gt;</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">└── </span><span style=\"color: #008000; text-decoration-color: #008000\">MicroGPT</span><span style=\"color: #808000; text-decoration-color: #808000\">(_orig_mod)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleDict</span><span style=\"color: #808000; text-decoration-color: #808000\">(transformer)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wte)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[50304,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wpe)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[256,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleList</span><span style=\"color: #808000; text-decoration-color: #808000\">(h)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │   └── </span>💠 <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Block</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(0-5)&lt;🦜:1770240x6&gt;</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┣━━ </span>💠 <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">LayerNorm</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(ln_1,ln_2)&lt;🦜:384x2&gt;|</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">CausalSelfAttention</span><span style=\"color: #808000; text-decoration-color: #808000\">(attn)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┃   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_attn)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1152,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┃   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">MLP</span><span style=\"color: #808000; text-decoration-color: #808000\">(mlp)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_fc)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1536,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,1536]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   └── </span><span style=\"color: #008000; text-decoration-color: #008000\">LayerNorm</span><span style=\"color: #808000; text-decoration-color: #808000\">(ln_f)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    └── </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(lm_head)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[50304,384]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m🌳 OptimizedModule<all params:30036864>\u001b[0m\n",
       "\u001b[38;5;37m└── \u001b[0m\u001b[32mMicroGPT\u001b[0m\u001b[33m(_orig_mod)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mModuleDict\u001b[0m\u001b[33m(transformer)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wte)|\u001b[0m\u001b[94mweight[50304,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wpe)|\u001b[0m\u001b[94mweight[256,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mModuleList\u001b[0m\u001b[33m(h)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m└── \u001b[0m💠 \u001b]8;id=569227;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mBlock\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=615546;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(0-5)<🦜:1770240x6>\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┣━━ \u001b[0m💠 \u001b]8;id=118570;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mLayerNorm\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=420989;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(ln_1,ln_2)<🦜:384x2>|\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=157852;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[94mweight[384]\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┣━━ \u001b[0m\u001b[32mCausalSelfAttention\u001b[0m\u001b[33m(attn)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┃   \u001b[0m\u001b[38;5;37m┣━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_attn)|\u001b[0m\u001b[94mweight[1152,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┃   \u001b[0m\u001b[38;5;37m┗━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┗━━ \u001b[0m\u001b[32mMLP\u001b[0m\u001b[33m(mlp)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m┣━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_fc)|\u001b[0m\u001b[94mweight[1536,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m┗━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,1536]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m└── \u001b[0m\u001b[32mLayerNorm\u001b[0m\u001b[33m(ln_f)|\u001b[0m\u001b[94mweight[384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m└── \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(lm_head)|\u001b[0m\u001b[94mweight[50304,384]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hiq.vis import print_model\n",
    "print_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1e0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_dir):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, data_dir)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "269476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c8261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global iter_num, best_val_loss, model, optimizer\n",
    "    \n",
    "    # training loop\n",
    "    # data_dir will be set here after config is loaded\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    \n",
    "    # attempt to derive vocab_size from the dataset\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    \n",
    "    # update model_args with vocab_size if found\n",
    "    if meta_vocab_size is not None:\n",
    "        model_args['vocab_size'] = meta_vocab_size\n",
    "        # recreate the model with the correct vocab_size\n",
    "        gptconf = MicroGPTConfig(**model_args)\n",
    "        model = MicroGPT(gptconf)\n",
    "        model.to(device)\n",
    "        # recompile if needed\n",
    "        if compile:\n",
    "            print(\"recompiling the model with updated vocab_size...\")\n",
    "            unoptimized_model = model\n",
    "            model = torch.compile(model)\n",
    "        # reconfigure optimizer\n",
    "        optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "    \n",
    "    # Print the final model structure after all potential modifications\n",
    "    from hiq.vis import print_model\n",
    "    print_model(model)\n",
    "    \n",
    "    X, Y = get_batch('train', data_dir) # fetch the very first batch\n",
    "    t0 = time.time()\n",
    "    local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "    raw_model = model # no DDP wrapper needed\n",
    "    running_mfu = -1.0\n",
    "    while True:\n",
    "\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0 and master_process:\n",
    "            losses = estimate_loss(data_dir)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'model_args': model_args,\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': config,\n",
    "                    }\n",
    "                    print(f\"saving checkpoint to {out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "        if iter_num == 0 and eval_only:\n",
    "            break\n",
    "\n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y = get_batch('train', data_dir)\n",
    "            # backward pass, with gradient scaling if training in fp16\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0 and master_process:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "\n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1bbeda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to: /home/wukong/git.repo/pypi/microgpt/notebook/data/shakespeare_char\n",
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python -m microgpt.prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54aea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "number of parameters: 10.65M\n",
      "recompiling the model with updated vocab_size...\n",
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">🌳 OptimizedModule&lt;all params:10745088&gt;</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">└── </span><span style=\"color: #008000; text-decoration-color: #008000\">MicroGPT</span><span style=\"color: #808000; text-decoration-color: #808000\">(_orig_mod)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleDict</span><span style=\"color: #808000; text-decoration-color: #808000\">(transformer)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wte)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[65,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">Embedding</span><span style=\"color: #808000; text-decoration-color: #808000\">(wpe)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[256,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   ├── </span><span style=\"color: #008000; text-decoration-color: #008000\">ModuleList</span><span style=\"color: #808000; text-decoration-color: #808000\">(h)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │   └── </span>💠 <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Block</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(0-5)&lt;🦜:1770240x6&gt;</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┣━━ </span>💠 <a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">LayerNorm</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #008000; text-decoration-color: #008000\">(ln_1,ln_2)&lt;🦜:384x2&gt;|</span></a><a href=\"/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span></a>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">CausalSelfAttention</span><span style=\"color: #808000; text-decoration-color: #808000\">(attn)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┃   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_attn)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1152,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┃   </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">MLP</span><span style=\"color: #808000; text-decoration-color: #808000\">(mlp)</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┣━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_fc)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[1536,384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   │       </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">    </span><span style=\"color: #00afaf; text-decoration-color: #00afaf\">┗━━ </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(c_proj)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384,1536]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    │   └── </span><span style=\"color: #008000; text-decoration-color: #008000\">LayerNorm</span><span style=\"color: #808000; text-decoration-color: #808000\">(ln_f)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[384]</span>\n",
       "<span style=\"color: #00afaf; text-decoration-color: #00afaf\">    └── </span><span style=\"color: #008000; text-decoration-color: #008000\">Linear</span><span style=\"color: #808000; text-decoration-color: #808000\">(lm_head)|</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">weight[65,384]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m🌳 OptimizedModule<all params:10745088>\u001b[0m\n",
       "\u001b[38;5;37m└── \u001b[0m\u001b[32mMicroGPT\u001b[0m\u001b[33m(_orig_mod)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mModuleDict\u001b[0m\u001b[33m(transformer)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wte)|\u001b[0m\u001b[94mweight[65,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mEmbedding\u001b[0m\u001b[33m(wpe)|\u001b[0m\u001b[94mweight[256,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m├── \u001b[0m\u001b[32mModuleList\u001b[0m\u001b[33m(h)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m└── \u001b[0m💠 \u001b]8;id=109338;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mBlock\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=477708;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(0-5)<🦜:1770240x6>\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┣━━ \u001b[0m💠 \u001b]8;id=435315;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[1;38;5;201mLayerNorm\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=377079;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[32m(ln_1,ln_2)<🦜:384x2>|\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=517943;/home/wukong/git.repo/pypi/microgpt/microgpt/model.py\u001b\\\u001b[94mweight[384]\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┣━━ \u001b[0m\u001b[32mCausalSelfAttention\u001b[0m\u001b[33m(attn)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┃   \u001b[0m\u001b[38;5;37m┣━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_attn)|\u001b[0m\u001b[94mweight[1152,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┃   \u001b[0m\u001b[38;5;37m┗━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m┗━━ \u001b[0m\u001b[32mMLP\u001b[0m\u001b[33m(mlp)\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m┣━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_fc)|\u001b[0m\u001b[94mweight[1536,384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m    \u001b[0m\u001b[38;5;201m    \u001b[0m\u001b[38;5;37m┗━━ \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(c_proj)|\u001b[0m\u001b[94mweight[384,1536]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m│   \u001b[0m\u001b[38;5;37m└── \u001b[0m\u001b[32mLayerNorm\u001b[0m\u001b[33m(ln_f)|\u001b[0m\u001b[94mweight[384]\u001b[0m\n",
       "\u001b[38;5;37m    \u001b[0m\u001b[38;5;37m└── \u001b[0m\u001b[32mLinear\u001b[0m\u001b[33m(lm_head)|\u001b[0m\u001b[94mweight[65,384]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3128, val loss 4.3080\n",
      "iter 0: loss 4.2885, time 17601.62ms, mfu -100.00%\n",
      "iter 10: loss 3.2350, time 15.02ms, mfu 24.80%\n",
      "iter 20: loss 2.8026, time 15.46ms, mfu 24.73%\n",
      "iter 30: loss 2.6264, time 15.08ms, mfu 24.73%\n",
      "iter 40: loss 2.5543, time 14.43ms, mfu 24.84%\n",
      "iter 50: loss 2.5380, time 15.37ms, mfu 24.78%\n",
      "iter 60: loss 2.5118, time 15.82ms, mfu 24.66%\n",
      "iter 70: loss 2.5038, time 15.27ms, mfu 24.63%\n",
      "iter 80: loss 2.4857, time 14.56ms, mfu 24.73%\n",
      "iter 90: loss 2.4883, time 15.26ms, mfu 24.70%\n",
      "iter 100: loss 2.4635, time 15.46ms, mfu 24.64%\n",
      "iter 110: loss 2.4543, time 15.25ms, mfu 24.62%\n",
      "iter 120: loss 2.4165, time 14.49ms, mfu 24.73%\n",
      "iter 130: loss 2.4358, time 15.25ms, mfu 24.70%\n",
      "iter 140: loss 2.4238, time 15.66ms, mfu 24.61%\n",
      "iter 150: loss 2.3916, time 15.22ms, mfu 24.60%\n",
      "iter 160: loss 2.3452, time 14.51ms, mfu 24.70%\n",
      "iter 170: loss 2.3514, time 15.52ms, mfu 24.63%\n",
      "iter 180: loss 2.3390, time 15.89ms, mfu 24.52%\n",
      "iter 190: loss 2.2973, time 15.05ms, mfu 24.54%\n",
      "iter 200: loss 2.2176, time 14.93ms, mfu 24.58%\n",
      "iter 210: loss 2.1800, time 15.19ms, mfu 24.58%\n",
      "iter 220: loss 2.1683, time 15.87ms, mfu 24.47%\n",
      "iter 230: loss 2.1146, time 15.34ms, mfu 24.45%\n",
      "iter 240: loss 2.0675, time 14.99ms, mfu 24.49%\n",
      "step 250: train loss 1.9785, val loss 2.0702\n",
      "saving checkpoint to out\n",
      "iter 250: loss 2.0150, time 2264.60ms, mfu 22.06%\n",
      "iter 260: loss 2.0088, time 15.34ms, mfu 22.28%\n",
      "iter 270: loss 1.9935, time 14.42ms, mfu 22.64%\n",
      "iter 280: loss 1.9598, time 15.50ms, mfu 22.78%\n",
      "iter 290: loss 1.9403, time 14.53ms, mfu 23.06%\n",
      "iter 300: loss 1.8839, time 15.46ms, mfu 23.17%\n",
      "iter 310: loss 1.8899, time 14.43ms, mfu 23.43%\n",
      "iter 320: loss 1.8851, time 15.52ms, mfu 23.49%\n",
      "iter 330: loss 1.8555, time 14.70ms, mfu 23.68%\n",
      "iter 340: loss 1.8126, time 15.60ms, mfu 23.70%\n",
      "iter 350: loss 1.8121, time 14.87ms, mfu 23.83%\n",
      "iter 360: loss 1.7801, time 15.49ms, mfu 23.86%\n",
      "iter 370: loss 1.7831, time 14.99ms, mfu 23.96%\n",
      "iter 380: loss 1.7911, time 14.92ms, mfu 24.06%\n",
      "iter 390: loss 1.7660, time 15.05ms, mfu 24.13%\n",
      "iter 400: loss 1.7359, time 14.69ms, mfu 24.25%\n",
      "iter 410: loss 1.7466, time 15.20ms, mfu 24.28%\n",
      "iter 420: loss 1.7200, time 14.53ms, mfu 24.42%\n",
      "iter 430: loss 1.7023, time 15.12ms, mfu 24.44%\n",
      "iter 440: loss 1.6733, time 14.56ms, mfu 24.55%\n",
      "iter 450: loss 1.6628, time 15.46ms, mfu 24.51%\n",
      "iter 460: loss 1.6694, time 14.77ms, mfu 24.58%\n",
      "iter 470: loss 1.6304, time 15.27ms, mfu 24.56%\n",
      "iter 480: loss 1.6294, time 15.34ms, mfu 24.54%\n",
      "iter 490: loss 1.6368, time 15.16ms, mfu 24.54%\n",
      "step 500: train loss 1.5337, val loss 1.7147\n",
      "saving checkpoint to out\n",
      "iter 500: loss 1.6292, time 2314.12ms, mfu 22.10%\n",
      "iter 510: loss 1.5828, time 15.26ms, mfu 22.34%\n",
      "iter 520: loss 1.6053, time 14.43ms, mfu 22.68%\n",
      "iter 530: loss 1.5950, time 15.21ms, mfu 22.87%\n",
      "iter 540: loss 1.5360, time 14.91ms, mfu 23.08%\n",
      "iter 550: loss 1.5964, time 15.51ms, mfu 23.17%\n",
      "iter 560: loss 1.5507, time 14.79ms, mfu 23.38%\n",
      "iter 570: loss 1.5695, time 15.39ms, mfu 23.46%\n",
      "iter 580: loss 1.5366, time 15.01ms, mfu 23.59%\n",
      "iter 590: loss 1.5836, time 15.13ms, mfu 23.70%\n",
      "iter 600: loss 1.5254, time 15.03ms, mfu 23.81%\n",
      "iter 610: loss 1.5142, time 15.20ms, mfu 23.88%\n",
      "iter 620: loss 1.5029, time 15.13ms, mfu 23.95%\n",
      "iter 630: loss 1.5110, time 14.83ms, mfu 24.07%\n",
      "iter 640: loss 1.4777, time 15.97ms, mfu 24.00%\n",
      "iter 650: loss 1.5175, time 14.44ms, mfu 24.18%\n",
      "iter 660: loss 1.4911, time 15.16ms, mfu 24.22%\n",
      "iter 670: loss 1.5120, time 14.46ms, mfu 24.37%\n",
      "iter 680: loss 1.4527, time 16.21ms, mfu 24.23%\n",
      "iter 690: loss 1.4697, time 14.99ms, mfu 24.30%\n",
      "iter 700: loss 1.4742, time 15.55ms, mfu 24.26%\n",
      "iter 710: loss 1.4637, time 15.08ms, mfu 24.31%\n",
      "iter 720: loss 1.4481, time 16.08ms, mfu 24.19%\n",
      "iter 730: loss 1.4579, time 14.87ms, mfu 24.28%\n",
      "iter 740: loss 1.4354, time 15.20ms, mfu 24.30%\n",
      "step 750: train loss 1.3591, val loss 1.5788\n",
      "saving checkpoint to out\n",
      "iter 750: loss 1.4365, time 2314.65ms, mfu 21.89%\n",
      "iter 760: loss 1.4362, time 14.95ms, mfu 22.19%\n",
      "iter 770: loss 1.4036, time 15.83ms, mfu 22.33%\n",
      "iter 780: loss 1.4068, time 15.21ms, mfu 22.55%\n",
      "iter 790: loss 1.4165, time 14.51ms, mfu 22.86%\n",
      "iter 800: loss 1.4282, time 15.14ms, mfu 23.04%\n",
      "iter 810: loss 1.4268, time 15.39ms, mfu 23.15%\n",
      "iter 820: loss 1.3968, time 15.64ms, mfu 23.22%\n",
      "iter 830: loss 1.4353, time 14.77ms, mfu 23.42%\n",
      "iter 840: loss 1.4015, time 15.12ms, mfu 23.54%\n",
      "iter 850: loss 1.3803, time 15.09ms, mfu 23.66%\n",
      "iter 860: loss 1.3874, time 15.23ms, mfu 23.74%\n",
      "iter 870: loss 1.4087, time 15.14ms, mfu 23.83%\n",
      "iter 880: loss 1.3699, time 15.31ms, mfu 23.88%\n",
      "iter 890: loss 1.3922, time 14.94ms, mfu 23.99%\n",
      "iter 900: loss 1.3701, time 15.20ms, mfu 24.04%\n",
      "iter 910: loss 1.3783, time 14.91ms, mfu 24.13%\n",
      "iter 920: loss 1.3705, time 15.26ms, mfu 24.16%\n",
      "iter 930: loss 1.3349, time 14.97ms, mfu 24.23%\n",
      "iter 940: loss 1.3318, time 14.98ms, mfu 24.30%\n",
      "iter 950: loss 1.3620, time 21.35ms, mfu 23.61%\n",
      "iter 960: loss 1.3484, time 15.05ms, mfu 23.73%\n",
      "iter 970: loss 1.3501, time 14.66ms, mfu 23.90%\n",
      "iter 980: loss 1.3566, time 15.05ms, mfu 23.98%\n",
      "iter 990: loss 1.3460, time 15.24ms, mfu 24.03%\n",
      "step 1000: train loss 1.2667, val loss 1.5228\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 1.3576, time 2307.41ms, mfu 21.64%\n",
      "iter 1010: loss 1.3254, time 14.56ms, mfu 22.04%\n",
      "iter 1020: loss 1.3454, time 14.83ms, mfu 22.35%\n",
      "iter 1030: loss 1.3688, time 14.59ms, mfu 22.67%\n",
      "iter 1040: loss 1.3321, time 15.46ms, mfu 22.81%\n",
      "iter 1050: loss 1.3230, time 14.56ms, mfu 23.09%\n",
      "iter 1060: loss 1.3571, time 15.41ms, mfu 23.20%\n",
      "iter 1070: loss 1.3420, time 14.89ms, mfu 23.38%\n",
      "iter 1080: loss 1.3211, time 15.06ms, mfu 23.52%\n",
      "iter 1090: loss 1.3105, time 15.31ms, mfu 23.60%\n",
      "iter 1100: loss 1.3007, time 14.99ms, mfu 23.73%\n",
      "iter 1110: loss 1.3207, time 15.54ms, mfu 23.75%\n",
      "iter 1120: loss 1.2991, time 14.35ms, mfu 23.97%\n",
      "iter 1130: loss 1.2902, time 15.46ms, mfu 23.98%\n",
      "iter 1140: loss 1.2993, time 14.82ms, mfu 24.10%\n",
      "iter 1150: loss 1.3046, time 15.54ms, mfu 24.09%\n",
      "iter 1160: loss 1.2913, time 15.00ms, mfu 24.16%\n",
      "iter 1170: loss 1.2776, time 15.31ms, mfu 24.18%\n",
      "iter 1180: loss 1.2780, time 15.07ms, mfu 24.23%\n",
      "iter 1190: loss 1.2963, time 14.96ms, mfu 24.30%\n",
      "iter 1200: loss 1.2716, time 15.09ms, mfu 24.34%\n",
      "iter 1210: loss 1.2555, time 14.93ms, mfu 24.40%\n",
      "iter 1220: loss 1.2587, time 15.22ms, mfu 24.41%\n",
      "iter 1230: loss 1.2685, time 14.57ms, mfu 24.53%\n",
      "iter 1240: loss 1.2796, time 15.40ms, mfu 24.49%\n",
      "step 1250: train loss 1.2028, val loss 1.5007\n",
      "saving checkpoint to out\n",
      "iter 1250: loss 1.2682, time 2296.73ms, mfu 22.06%\n",
      "iter 1260: loss 1.2565, time 15.16ms, mfu 22.31%\n",
      "iter 1270: loss 1.2891, time 14.50ms, mfu 22.65%\n",
      "iter 1280: loss 1.2759, time 14.70ms, mfu 22.92%\n",
      "iter 1290: loss 1.2798, time 14.81ms, mfu 23.14%\n",
      "iter 1300: loss 1.2376, time 15.22ms, mfu 23.28%\n",
      "iter 1310: loss 1.2269, time 14.99ms, mfu 23.44%\n",
      "iter 1320: loss 1.2547, time 15.04ms, mfu 23.57%\n",
      "iter 1330: loss 1.2814, time 14.57ms, mfu 23.77%\n",
      "iter 1340: loss 1.2391, time 15.28ms, mfu 23.83%\n",
      "iter 1350: loss 1.2627, time 14.77ms, mfu 23.97%\n",
      "iter 1360: loss 1.2717, time 15.02ms, mfu 24.06%\n",
      "iter 1370: loss 1.2694, time 15.16ms, mfu 24.11%\n",
      "iter 1380: loss 1.2341, time 15.09ms, mfu 24.17%\n",
      "iter 1390: loss 1.2518, time 15.84ms, mfu 24.10%\n",
      "iter 1400: loss 1.2450, time 14.73ms, mfu 24.22%\n",
      "iter 1410: loss 1.2355, time 15.11ms, mfu 24.26%\n",
      "iter 1420: loss 1.2224, time 14.47ms, mfu 24.41%\n",
      "iter 1430: loss 1.2506, time 15.31ms, mfu 24.40%\n",
      "iter 1440: loss 1.2450, time 15.01ms, mfu 24.45%\n",
      "iter 1450: loss 1.2429, time 15.46ms, mfu 24.41%\n",
      "iter 1460: loss 1.1865, time 15.19ms, mfu 24.42%\n",
      "iter 1470: loss 1.2284, time 15.26ms, mfu 24.42%\n",
      "iter 1480: loss 1.2222, time 14.75ms, mfu 24.51%\n",
      "iter 1490: loss 1.2231, time 14.84ms, mfu 24.57%\n",
      "step 1500: train loss 1.1390, val loss 1.4724\n",
      "saving checkpoint to out\n",
      "iter 1500: loss 1.2064, time 2312.20ms, mfu 22.13%\n",
      "iter 1510: loss 1.2212, time 14.90ms, mfu 22.41%\n",
      "iter 1520: loss 1.2278, time 15.12ms, mfu 22.64%\n",
      "iter 1530: loss 1.2018, time 14.72ms, mfu 22.90%\n",
      "iter 1540: loss 1.1824, time 15.35ms, mfu 23.04%\n",
      "iter 1550: loss 1.2041, time 14.59ms, mfu 23.29%\n",
      "iter 1560: loss 1.2041, time 15.32ms, mfu 23.39%\n",
      "iter 1570: loss 1.1986, time 14.68ms, mfu 23.59%\n",
      "iter 1580: loss 1.2130, time 15.49ms, mfu 23.64%\n",
      "iter 1590: loss 1.2134, time 15.30ms, mfu 23.71%\n",
      "iter 1600: loss 1.1948, time 14.96ms, mfu 23.83%\n",
      "iter 1610: loss 1.2152, time 14.90ms, mfu 23.95%\n",
      "iter 1620: loss 1.2122, time 14.46ms, mfu 24.13%\n",
      "iter 1630: loss 1.1963, time 14.78ms, mfu 24.24%\n",
      "iter 1640: loss 1.1791, time 14.50ms, mfu 24.38%\n",
      "iter 1650: loss 1.2005, time 15.24ms, mfu 24.39%\n",
      "iter 1660: loss 1.1946, time 15.25ms, mfu 24.39%\n",
      "iter 1670: loss 1.2175, time 15.28ms, mfu 24.39%\n",
      "iter 1680: loss 1.1828, time 15.22ms, mfu 24.40%\n",
      "iter 1690: loss 1.1891, time 14.72ms, mfu 24.49%\n",
      "iter 1700: loss 1.1577, time 14.74ms, mfu 24.57%\n",
      "iter 1710: loss 1.1927, time 14.48ms, mfu 24.69%\n",
      "iter 1720: loss 1.1882, time 15.25ms, mfu 24.66%\n",
      "iter 1730: loss 1.1555, time 14.88ms, mfu 24.70%\n",
      "iter 1740: loss 1.2047, time 15.46ms, mfu 24.64%\n",
      "step 1750: train loss 1.0979, val loss 1.4624\n",
      "saving checkpoint to out\n",
      "iter 1750: loss 1.1555, time 2293.55ms, mfu 22.19%\n",
      "iter 1760: loss 1.2333, time 14.98ms, mfu 22.46%\n",
      "iter 1770: loss 1.1938, time 14.51ms, mfu 22.78%\n",
      "iter 1780: loss 1.1964, time 15.26ms, mfu 22.95%\n",
      "iter 1790: loss 1.1647, time 15.02ms, mfu 23.13%\n",
      "iter 1800: loss 1.1551, time 15.40ms, mfu 23.24%\n",
      "iter 1810: loss 1.1393, time 15.17ms, mfu 23.37%\n",
      "iter 1820: loss 1.1448, time 15.44ms, mfu 23.45%\n",
      "iter 1830: loss 1.1456, time 14.86ms, mfu 23.61%\n",
      "iter 1840: loss 1.1694, time 14.66ms, mfu 23.79%\n",
      "iter 1850: loss 1.1868, time 15.07ms, mfu 23.89%\n",
      "iter 1860: loss 1.1521, time 14.76ms, mfu 24.02%\n",
      "iter 1870: loss 1.1871, time 14.92ms, mfu 24.12%\n",
      "iter 1880: loss 1.1367, time 14.66ms, mfu 24.25%\n",
      "iter 1890: loss 1.1399, time 15.30ms, mfu 24.26%\n",
      "iter 1900: loss 1.1338, time 14.90ms, mfu 24.33%\n",
      "iter 1910: loss 1.1608, time 15.51ms, mfu 24.30%\n",
      "iter 1920: loss 1.1508, time 15.34ms, mfu 24.30%\n",
      "iter 1930: loss 1.1827, time 15.52ms, mfu 24.27%\n",
      "iter 1940: loss 1.1534, time 15.23ms, mfu 24.29%\n",
      "iter 1950: loss 1.1318, time 15.02ms, mfu 24.34%\n",
      "iter 1960: loss 1.1441, time 15.05ms, mfu 24.38%\n",
      "iter 1970: loss 1.1633, time 14.52ms, mfu 24.51%\n",
      "iter 1980: loss 1.1571, time 15.22ms, mfu 24.51%\n",
      "iter 1990: loss 1.1287, time 14.67ms, mfu 24.60%\n",
      "step 2000: train loss 1.0530, val loss 1.4754\n",
      "iter 2000: loss 1.1594, time 2180.25ms, mfu 22.16%\n",
      "iter 2010: loss 1.1319, time 15.10ms, mfu 22.41%\n",
      "iter 2020: loss 1.1635, time 15.46ms, mfu 22.58%\n",
      "iter 2030: loss 1.1407, time 15.22ms, mfu 22.77%\n",
      "iter 2040: loss 1.1341, time 14.60ms, mfu 23.04%\n",
      "iter 2050: loss 1.1384, time 15.20ms, mfu 23.19%\n",
      "iter 2060: loss 1.1193, time 15.83ms, mfu 23.23%\n",
      "iter 2070: loss 1.1154, time 15.40ms, mfu 23.32%\n",
      "iter 2080: loss 1.1143, time 14.99ms, mfu 23.48%\n",
      "iter 2090: loss 1.0978, time 15.05ms, mfu 23.61%\n",
      "iter 2100: loss 1.0911, time 16.28ms, mfu 23.53%\n",
      "iter 2110: loss 1.1296, time 15.16ms, mfu 23.64%\n",
      "iter 2120: loss 1.1162, time 15.09ms, mfu 23.75%\n",
      "iter 2130: loss 1.1072, time 14.63ms, mfu 23.92%\n",
      "iter 2140: loss 1.0926, time 16.02ms, mfu 23.85%\n",
      "iter 2150: loss 1.1200, time 14.85ms, mfu 23.98%\n",
      "iter 2160: loss 1.1134, time 15.25ms, mfu 24.02%\n",
      "iter 2170: loss 1.1355, time 14.91ms, mfu 24.12%\n",
      "iter 2180: loss 1.0982, time 15.32ms, mfu 24.14%\n",
      "iter 2190: loss 1.1266, time 15.33ms, mfu 24.16%\n",
      "iter 2200: loss 1.0942, time 15.16ms, mfu 24.20%\n",
      "iter 2210: loss 1.0685, time 15.32ms, mfu 24.21%\n",
      "iter 2220: loss 1.0870, time 14.91ms, mfu 24.29%\n",
      "iter 2230: loss 1.1065, time 15.39ms, mfu 24.28%\n",
      "iter 2240: loss 1.0973, time 14.95ms, mfu 24.35%\n",
      "step 2250: train loss 1.0019, val loss 1.4762\n",
      "iter 2250: loss 1.1191, time 2173.96ms, mfu 21.93%\n",
      "iter 2260: loss 1.0708, time 15.49ms, mfu 22.14%\n",
      "iter 2270: loss 1.0839, time 15.00ms, mfu 22.41%\n",
      "iter 2280: loss 1.0951, time 15.19ms, mfu 22.62%\n",
      "iter 2290: loss 1.0853, time 14.99ms, mfu 22.85%\n",
      "iter 2300: loss 1.0704, time 15.02ms, mfu 23.04%\n",
      "iter 2310: loss 1.0920, time 14.96ms, mfu 23.23%\n",
      "iter 2320: loss 1.0513, time 14.74ms, mfu 23.43%\n",
      "iter 2330: loss 1.1149, time 15.09ms, mfu 23.56%\n",
      "iter 2340: loss 1.1001, time 14.70ms, mfu 23.74%\n",
      "iter 2350: loss 1.0868, time 15.28ms, mfu 23.80%\n",
      "iter 2360: loss 1.1140, time 15.28ms, mfu 23.86%\n",
      "iter 2370: loss 1.0979, time 15.08ms, mfu 23.95%\n",
      "iter 2380: loss 1.0605, time 15.18ms, mfu 24.01%\n",
      "iter 2390: loss 1.0888, time 15.49ms, mfu 24.01%\n",
      "iter 2400: loss 1.0996, time 15.01ms, mfu 24.09%\n",
      "iter 2410: loss 1.0687, time 15.48ms, mfu 24.09%\n",
      "iter 2420: loss 1.0765, time 15.56ms, mfu 24.08%\n",
      "iter 2430: loss 1.1026, time 15.19ms, mfu 24.12%\n",
      "iter 2440: loss 1.1003, time 15.05ms, mfu 24.18%\n",
      "iter 2450: loss 1.0934, time 14.54ms, mfu 24.33%\n",
      "iter 2460: loss 1.0659, time 15.31ms, mfu 24.33%\n",
      "iter 2470: loss 1.0402, time 14.90ms, mfu 24.40%\n",
      "iter 2480: loss 1.0187, time 15.25ms, mfu 24.40%\n",
      "iter 2490: loss 1.0760, time 15.13ms, mfu 24.42%\n",
      "step 2500: train loss 0.9545, val loss 1.4941\n",
      "iter 2500: loss 1.0517, time 2187.72ms, mfu 22.00%\n",
      "iter 2510: loss 1.0665, time 15.20ms, mfu 22.25%\n",
      "iter 2520: loss 1.0990, time 15.35ms, mfu 22.45%\n",
      "iter 2530: loss 1.0623, time 15.19ms, mfu 22.66%\n",
      "iter 2540: loss 1.0435, time 15.05ms, mfu 22.87%\n",
      "iter 2550: loss 1.0541, time 14.74ms, mfu 23.11%\n",
      "iter 2560: loss 1.0672, time 15.32ms, mfu 23.23%\n",
      "iter 2570: loss 1.0549, time 14.93ms, mfu 23.41%\n",
      "iter 2580: loss 1.0451, time 15.28ms, mfu 23.50%\n",
      "iter 2590: loss 1.0331, time 15.23ms, mfu 23.60%\n",
      "iter 2600: loss 1.0685, time 14.76ms, mfu 23.76%\n",
      "iter 2610: loss 1.0738, time 15.29ms, mfu 23.83%\n",
      "iter 2620: loss 1.0783, time 14.84ms, mfu 23.95%\n",
      "iter 2630: loss 1.0297, time 15.16ms, mfu 24.02%\n",
      "iter 2640: loss 1.0461, time 14.58ms, mfu 24.17%\n",
      "iter 2650: loss 1.0802, time 15.01ms, mfu 24.24%\n",
      "iter 2660: loss 1.0293, time 14.68ms, mfu 24.35%\n",
      "iter 2670: loss 1.0432, time 15.44ms, mfu 24.33%\n",
      "iter 2680: loss 1.0300, time 15.03ms, mfu 24.38%\n",
      "iter 2690: loss 1.0297, time 15.48ms, mfu 24.35%\n",
      "iter 2700: loss 1.0481, time 15.27ms, mfu 24.35%\n",
      "iter 2710: loss 1.0285, time 14.99ms, mfu 24.40%\n",
      "iter 2720: loss 1.0312, time 15.00ms, mfu 24.45%\n",
      "iter 2730: loss 1.0126, time 14.47ms, mfu 24.58%\n",
      "iter 2740: loss 1.0119, time 15.15ms, mfu 24.58%\n",
      "step 2750: train loss 0.9108, val loss 1.5120\n",
      "iter 2750: loss 1.0503, time 2197.00ms, mfu 22.14%\n",
      "iter 2760: loss 1.0401, time 15.23ms, mfu 22.37%\n",
      "iter 2770: loss 1.0264, time 15.03ms, mfu 22.61%\n",
      "iter 2780: loss 1.0197, time 15.58ms, mfu 22.74%\n",
      "iter 2790: loss 1.0047, time 15.93ms, mfu 22.81%\n",
      "iter 2800: loss 1.0314, time 15.13ms, mfu 22.99%\n",
      "iter 2810: loss 1.0245, time 14.95ms, mfu 23.18%\n",
      "iter 2820: loss 1.0322, time 14.53ms, mfu 23.43%\n",
      "iter 2830: loss 1.0370, time 15.19ms, mfu 23.54%\n",
      "iter 2840: loss 1.0185, time 15.21ms, mfu 23.64%\n",
      "iter 2850: loss 1.0163, time 15.31ms, mfu 23.71%\n",
      "iter 2860: loss 1.0377, time 15.23ms, mfu 23.78%\n",
      "iter 2870: loss 1.0310, time 14.89ms, mfu 23.90%\n",
      "iter 2880: loss 0.9842, time 15.00ms, mfu 24.00%\n",
      "iter 2890: loss 0.9983, time 14.74ms, mfu 24.13%\n",
      "iter 2900: loss 1.0239, time 15.17ms, mfu 24.17%\n",
      "iter 2910: loss 1.0044, time 15.07ms, mfu 24.23%\n",
      "iter 2920: loss 1.0047, time 15.41ms, mfu 24.22%\n",
      "iter 2930: loss 0.9839, time 15.25ms, mfu 24.24%\n",
      "iter 2940: loss 0.9826, time 15.54ms, mfu 24.22%\n",
      "iter 2950: loss 0.9832, time 15.27ms, mfu 24.23%\n",
      "iter 2960: loss 0.9955, time 14.85ms, mfu 24.32%\n",
      "iter 2970: loss 0.9799, time 15.07ms, mfu 24.36%\n",
      "iter 2980: loss 0.9816, time 14.57ms, mfu 24.48%\n",
      "iter 2990: loss 1.0043, time 15.57ms, mfu 24.43%\n",
      "step 3000: train loss 0.8598, val loss 1.5336\n",
      "iter 3000: loss 0.9682, time 2199.73ms, mfu 22.00%\n",
      "iter 3010: loss 0.9791, time 15.30ms, mfu 22.24%\n",
      "iter 3020: loss 1.0135, time 15.28ms, mfu 22.45%\n",
      "iter 3030: loss 0.9956, time 14.72ms, mfu 22.74%\n",
      "iter 3040: loss 0.9530, time 15.33ms, mfu 22.89%\n",
      "iter 3050: loss 0.9571, time 14.52ms, mfu 23.17%\n",
      "iter 3060: loss 0.9923, time 15.38ms, mfu 23.28%\n",
      "iter 3070: loss 0.9774, time 14.87ms, mfu 23.45%\n",
      "iter 3080: loss 0.9630, time 15.22ms, mfu 23.56%\n",
      "iter 3090: loss 0.9717, time 14.77ms, mfu 23.73%\n",
      "iter 3100: loss 0.9844, time 15.43ms, mfu 23.77%\n",
      "iter 3110: loss 0.9755, time 15.12ms, mfu 23.86%\n",
      "iter 3120: loss 0.9757, time 15.36ms, mfu 23.90%\n",
      "iter 3130: loss 0.9812, time 14.79ms, mfu 24.03%\n",
      "iter 3140: loss 0.9702, time 14.60ms, mfu 24.18%\n",
      "iter 3150: loss 0.9492, time 14.93ms, mfu 24.25%\n",
      "iter 3160: loss 0.9808, time 14.48ms, mfu 24.40%\n",
      "iter 3170: loss 0.9792, time 15.01ms, mfu 24.44%\n",
      "iter 3180: loss 0.9319, time 15.23ms, mfu 24.45%\n",
      "iter 3190: loss 0.9760, time 15.33ms, mfu 24.43%\n",
      "iter 3200: loss 0.9716, time 15.26ms, mfu 24.43%\n",
      "iter 3210: loss 0.9486, time 15.04ms, mfu 24.47%\n",
      "iter 3220: loss 0.9823, time 14.81ms, mfu 24.53%\n",
      "iter 3230: loss 0.9629, time 14.47ms, mfu 24.66%\n",
      "iter 3240: loss 0.9607, time 15.15ms, mfu 24.65%\n",
      "step 3250: train loss 0.8137, val loss 1.5642\n",
      "iter 3250: loss 0.9505, time 2184.33ms, mfu 22.20%\n",
      "iter 3260: loss 0.9346, time 15.05ms, mfu 22.46%\n",
      "iter 3270: loss 0.9502, time 14.84ms, mfu 22.72%\n",
      "iter 3280: loss 0.9287, time 14.74ms, mfu 22.98%\n",
      "iter 3290: loss 0.9516, time 15.32ms, mfu 23.11%\n",
      "iter 3300: loss 0.9525, time 14.84ms, mfu 23.31%\n",
      "iter 3310: loss 0.9607, time 15.47ms, mfu 23.39%\n",
      "iter 3320: loss 0.9485, time 15.15ms, mfu 23.51%\n",
      "iter 3330: loss 0.9559, time 15.23ms, mfu 23.60%\n",
      "iter 3340: loss 0.9636, time 14.95ms, mfu 23.74%\n",
      "iter 3350: loss 0.9640, time 14.89ms, mfu 23.87%\n",
      "iter 3360: loss 0.9416, time 14.95ms, mfu 23.97%\n",
      "iter 3370: loss 0.9676, time 14.49ms, mfu 24.14%\n",
      "iter 3380: loss 0.9366, time 16.03ms, mfu 24.06%\n",
      "iter 3390: loss 0.9258, time 15.07ms, mfu 24.12%\n",
      "iter 3400: loss 0.9231, time 15.36ms, mfu 24.13%\n",
      "iter 3410: loss 0.9343, time 15.19ms, mfu 24.17%\n",
      "iter 3420: loss 0.9076, time 16.02ms, mfu 24.08%\n",
      "iter 3430: loss 0.9356, time 15.16ms, mfu 24.13%\n",
      "iter 3440: loss 0.9189, time 14.97ms, mfu 24.21%\n",
      "iter 3450: loss 0.9525, time 15.12ms, mfu 24.25%\n",
      "iter 3460: loss 0.9589, time 15.75ms, mfu 24.19%\n",
      "iter 3470: loss 0.9213, time 15.58ms, mfu 24.16%\n",
      "iter 3480: loss 0.9151, time 15.26ms, mfu 24.19%\n",
      "iter 3490: loss 0.9107, time 15.49ms, mfu 24.18%\n",
      "step 3500: train loss 0.7707, val loss 1.5834\n",
      "iter 3500: loss 0.9105, time 2200.68ms, mfu 21.78%\n",
      "iter 3510: loss 0.9346, time 14.78ms, mfu 22.12%\n",
      "iter 3520: loss 0.8852, time 15.24ms, mfu 22.35%\n",
      "iter 3530: loss 0.9029, time 15.11ms, mfu 22.58%\n",
      "iter 3540: loss 0.9268, time 15.42ms, mfu 22.74%\n",
      "iter 3550: loss 0.9336, time 15.37ms, mfu 22.89%\n",
      "iter 3560: loss 0.9445, time 15.10ms, mfu 23.07%\n",
      "iter 3570: loss 0.9093, time 15.03ms, mfu 23.24%\n",
      "iter 3580: loss 0.9250, time 14.58ms, mfu 23.47%\n",
      "iter 3590: loss 0.9284, time 15.01ms, mfu 23.61%\n",
      "iter 3600: loss 0.9536, time 15.08ms, mfu 23.72%\n",
      "iter 3610: loss 0.9058, time 15.46ms, mfu 23.76%\n",
      "iter 3620: loss 0.9087, time 15.31ms, mfu 23.81%\n",
      "iter 3630: loss 0.9176, time 15.27ms, mfu 23.87%\n",
      "iter 3640: loss 0.9174, time 15.22ms, mfu 23.93%\n",
      "iter 3650: loss 0.9151, time 14.86ms, mfu 24.05%\n",
      "iter 3660: loss 0.8759, time 14.94ms, mfu 24.14%\n",
      "iter 3670: loss 0.9106, time 14.62ms, mfu 24.27%\n",
      "iter 3680: loss 0.9124, time 15.25ms, mfu 24.29%\n",
      "iter 3690: loss 0.8985, time 14.73ms, mfu 24.39%\n",
      "iter 3700: loss 0.8956, time 15.46ms, mfu 24.36%\n",
      "iter 3710: loss 0.9020, time 15.35ms, mfu 24.35%\n",
      "iter 3720: loss 0.8858, time 14.95ms, mfu 24.41%\n",
      "iter 3730: loss 0.9121, time 14.91ms, mfu 24.47%\n",
      "iter 3740: loss 0.8980, time 14.58ms, mfu 24.58%\n",
      "step 3750: train loss 0.7332, val loss 1.6092\n",
      "iter 3750: loss 0.9083, time 2180.79ms, mfu 22.13%\n",
      "iter 3760: loss 0.8911, time 14.91ms, mfu 22.42%\n",
      "iter 3770: loss 0.9179, time 15.53ms, mfu 22.58%\n",
      "iter 3780: loss 0.8775, time 15.37ms, mfu 22.74%\n",
      "iter 3790: loss 0.8782, time 14.66ms, mfu 23.01%\n",
      "iter 3800: loss 0.8913, time 14.68ms, mfu 23.25%\n",
      "iter 3810: loss 0.8786, time 14.66ms, mfu 23.46%\n",
      "iter 3820: loss 0.8918, time 15.27ms, mfu 23.56%\n",
      "iter 3830: loss 0.8691, time 15.14ms, mfu 23.66%\n",
      "iter 3840: loss 0.9157, time 15.42ms, mfu 23.71%\n",
      "iter 3850: loss 0.8889, time 14.95ms, mfu 23.83%\n",
      "iter 3860: loss 0.8784, time 15.03ms, mfu 23.93%\n",
      "iter 3870: loss 0.8733, time 14.90ms, mfu 24.04%\n",
      "iter 3880: loss 0.8849, time 14.54ms, mfu 24.20%\n",
      "iter 3890: loss 0.8819, time 15.23ms, mfu 24.22%\n",
      "iter 3900: loss 0.8772, time 14.79ms, mfu 24.32%\n",
      "iter 3910: loss 0.9015, time 15.46ms, mfu 24.30%\n",
      "iter 3920: loss 0.8819, time 15.08ms, mfu 24.34%\n",
      "iter 3930: loss 0.8639, time 15.31ms, mfu 24.34%\n",
      "iter 3940: loss 0.8805, time 15.13ms, mfu 24.37%\n",
      "iter 3950: loss 0.8719, time 14.75ms, mfu 24.46%\n",
      "iter 3960: loss 0.8866, time 15.19ms, mfu 24.47%\n",
      "iter 3970: loss 0.8860, time 14.68ms, mfu 24.56%\n",
      "iter 3980: loss 0.8826, time 15.56ms, mfu 24.50%\n",
      "iter 3990: loss 0.8537, time 15.26ms, mfu 24.49%\n",
      "step 4000: train loss 0.6992, val loss 1.6331\n",
      "iter 4000: loss 0.8622, time 2175.56ms, mfu 22.06%\n",
      "iter 4010: loss 0.8744, time 14.53ms, mfu 22.42%\n",
      "iter 4020: loss 0.8790, time 15.08ms, mfu 22.65%\n",
      "iter 4030: loss 0.8883, time 15.28ms, mfu 22.82%\n",
      "iter 4040: loss 0.8803, time 15.19ms, mfu 22.99%\n",
      "iter 4050: loss 0.8728, time 15.46ms, mfu 23.10%\n",
      "iter 4060: loss 0.8617, time 15.02ms, mfu 23.27%\n",
      "iter 4070: loss 0.8484, time 14.93ms, mfu 23.44%\n",
      "iter 4080: loss 0.8416, time 14.81ms, mfu 23.61%\n",
      "iter 4090: loss 0.8715, time 15.43ms, mfu 23.67%\n",
      "iter 4100: loss 0.8494, time 15.07ms, mfu 23.77%\n",
      "iter 4110: loss 0.8481, time 15.54ms, mfu 23.79%\n",
      "iter 4120: loss 0.8354, time 15.20ms, mfu 23.87%\n",
      "iter 4130: loss 0.8636, time 15.22ms, mfu 23.93%\n",
      "iter 4140: loss 0.8613, time 14.93ms, mfu 24.03%\n",
      "iter 4150: loss 0.8565, time 14.45ms, mfu 24.21%\n",
      "iter 4160: loss 0.8537, time 15.12ms, mfu 24.25%\n",
      "iter 4170: loss 0.8480, time 15.01ms, mfu 24.31%\n",
      "iter 4180: loss 0.8546, time 15.46ms, mfu 24.29%\n",
      "iter 4190: loss 0.8444, time 14.68ms, mfu 24.40%\n",
      "iter 4200: loss 0.8533, time 15.27ms, mfu 24.40%\n",
      "iter 4210: loss 0.8534, time 15.34ms, mfu 24.39%\n",
      "iter 4220: loss 0.8726, time 15.12ms, mfu 24.41%\n",
      "iter 4230: loss 0.8579, time 15.03ms, mfu 24.45%\n",
      "iter 4240: loss 0.8383, time 14.51ms, mfu 24.57%\n",
      "step 4250: train loss 0.6709, val loss 1.6539\n",
      "iter 4250: loss 0.8636, time 2178.51ms, mfu 22.13%\n",
      "iter 4260: loss 0.8505, time 14.76ms, mfu 22.45%\n",
      "iter 4270: loss 0.8422, time 15.45ms, mfu 22.61%\n",
      "iter 4280: loss 0.8858, time 15.23ms, mfu 22.80%\n",
      "iter 4290: loss 0.8506, time 15.35ms, mfu 22.95%\n",
      "iter 4300: loss 0.8297, time 15.25ms, mfu 23.09%\n",
      "iter 4310: loss 0.8354, time 15.24ms, mfu 23.23%\n",
      "iter 4320: loss 0.8485, time 15.08ms, mfu 23.38%\n",
      "iter 4330: loss 0.8518, time 14.93ms, mfu 23.54%\n",
      "iter 4340: loss 0.8642, time 14.70ms, mfu 23.72%\n",
      "iter 4350: loss 0.8465, time 14.49ms, mfu 23.92%\n",
      "iter 4360: loss 0.8476, time 15.20ms, mfu 23.98%\n",
      "iter 4370: loss 0.8640, time 14.95ms, mfu 24.07%\n",
      "iter 4380: loss 0.8396, time 15.10ms, mfu 24.13%\n",
      "iter 4390: loss 0.8548, time 14.89ms, mfu 24.22%\n",
      "iter 4400: loss 0.8199, time 15.30ms, mfu 24.23%\n",
      "iter 4410: loss 0.8327, time 15.16ms, mfu 24.27%\n",
      "iter 4420: loss 0.8232, time 15.38ms, mfu 24.26%\n",
      "iter 4430: loss 0.8275, time 15.08ms, mfu 24.31%\n",
      "iter 4440: loss 0.8254, time 15.18ms, mfu 24.33%\n",
      "iter 4450: loss 0.8256, time 15.30ms, mfu 24.34%\n",
      "iter 4460: loss 0.8250, time 15.32ms, mfu 24.34%\n",
      "iter 4470: loss 0.8391, time 15.24ms, mfu 24.35%\n",
      "iter 4480: loss 0.8469, time 15.53ms, mfu 24.31%\n",
      "iter 4490: loss 0.8195, time 15.30ms, mfu 24.32%\n",
      "step 4500: train loss 0.6446, val loss 1.6750\n",
      "iter 4500: loss 0.8440, time 2183.14ms, mfu 21.90%\n",
      "iter 4510: loss 0.8499, time 15.24ms, mfu 22.16%\n",
      "iter 4520: loss 0.8307, time 14.80ms, mfu 22.46%\n",
      "iter 4530: loss 0.8347, time 15.38ms, mfu 22.64%\n",
      "iter 4540: loss 0.8185, time 15.03ms, mfu 22.85%\n",
      "iter 4550: loss 0.8357, time 14.72ms, mfu 23.10%\n",
      "iter 4560: loss 0.8101, time 15.12ms, mfu 23.25%\n",
      "iter 4570: loss 0.8256, time 15.00ms, mfu 23.41%\n",
      "iter 4580: loss 0.8136, time 14.73ms, mfu 23.60%\n",
      "iter 4590: loss 0.8523, time 14.55ms, mfu 23.80%\n",
      "iter 4600: loss 0.8338, time 14.94ms, mfu 23.91%\n",
      "iter 4610: loss 0.8372, time 14.80ms, mfu 24.04%\n",
      "iter 4620: loss 0.8279, time 15.00ms, mfu 24.12%\n",
      "iter 4630: loss 0.8116, time 15.23ms, mfu 24.15%\n",
      "iter 4640: loss 0.7898, time 15.37ms, mfu 24.16%\n",
      "iter 4650: loss 0.8248, time 15.31ms, mfu 24.18%\n",
      "iter 4660: loss 0.8071, time 15.31ms, mfu 24.20%\n",
      "iter 4670: loss 0.8193, time 15.33ms, mfu 24.21%\n",
      "iter 4680: loss 0.8314, time 14.87ms, mfu 24.29%\n",
      "iter 4690: loss 0.8105, time 15.31ms, mfu 24.30%\n",
      "iter 4700: loss 0.8263, time 14.72ms, mfu 24.40%\n",
      "iter 4710: loss 0.8478, time 15.18ms, mfu 24.41%\n",
      "iter 4720: loss 0.8312, time 14.94ms, mfu 24.47%\n",
      "iter 4730: loss 0.8156, time 15.47ms, mfu 24.43%\n",
      "iter 4740: loss 0.8283, time 15.25ms, mfu 24.43%\n",
      "step 4750: train loss 0.6243, val loss 1.6866\n",
      "iter 4750: loss 0.8165, time 2182.62ms, mfu 22.00%\n",
      "iter 4760: loss 0.8153, time 16.07ms, mfu 22.12%\n",
      "iter 4770: loss 0.7984, time 14.82ms, mfu 22.42%\n",
      "iter 4780: loss 0.8144, time 14.78ms, mfu 22.70%\n",
      "iter 4790: loss 0.8162, time 14.93ms, mfu 22.93%\n",
      "iter 4800: loss 0.8065, time 15.83ms, mfu 22.99%\n",
      "iter 4810: loss 0.8005, time 14.82ms, mfu 23.21%\n",
      "iter 4820: loss 0.8238, time 15.17ms, mfu 23.34%\n",
      "iter 4830: loss 0.8314, time 15.02ms, mfu 23.49%\n",
      "iter 4840: loss 0.8084, time 16.31ms, mfu 23.42%\n",
      "iter 4850: loss 0.8271, time 15.30ms, mfu 23.52%\n",
      "iter 4860: loss 0.8127, time 15.52ms, mfu 23.57%\n",
      "iter 4870: loss 0.8076, time 15.29ms, mfu 23.65%\n",
      "iter 4880: loss 0.7946, time 16.48ms, mfu 23.54%\n",
      "iter 4890: loss 0.8166, time 15.30ms, mfu 23.62%\n",
      "iter 4900: loss 0.8170, time 15.11ms, mfu 23.73%\n",
      "iter 4910: loss 0.8198, time 14.95ms, mfu 23.85%\n",
      "iter 4920: loss 0.8053, time 14.57ms, mfu 24.02%\n",
      "iter 4930: loss 0.8112, time 15.33ms, mfu 24.05%\n",
      "iter 4940: loss 0.8017, time 14.85ms, mfu 24.15%\n",
      "iter 4950: loss 0.8353, time 15.32ms, mfu 24.17%\n",
      "iter 4960: loss 0.7693, time 15.20ms, mfu 24.20%\n",
      "iter 4970: loss 0.7937, time 15.16ms, mfu 24.24%\n",
      "iter 4980: loss 0.8313, time 15.24ms, mfu 24.26%\n",
      "iter 4990: loss 0.7859, time 14.90ms, mfu 24.34%\n",
      "step 5000: train loss 0.6108, val loss 1.6974\n",
      "iter 5000: loss 0.8265, time 2185.32ms, mfu 21.92%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7f51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /home/wukong/git.repo/pypi/microgpt/microgpt/pretrain/config.py\n",
      "Using out_dir: out\n",
      "Loading checkpoint from: out/ckpt.pt\n",
      "number of parameters: 10.65M\n",
      "Using data directory meta.pkl from: data/shakespeare_char/meta.pkl\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "All the bride with the sweet made of black and mage-day\n",
      "Beat the supposities of a bed,\n",
      "And that are away with himself thousand of spite,\n",
      "Making that he will be mine, send in the state,\n",
      "Give thee the noble benefit. Ah, no!\n",
      "Thou liest me with a special place, and to see thy face!\n",
      "I will not see the king Willow be Caius Marcius down.\n",
      "\n",
      "Third Messenger:\n",
      "A life that known not the sorrow is not to thy flesh than fear.\n",
      "\n",
      "MENENIUS:\n",
      "Thou art too discharged with a helperous court\n",
      "and no very poor court. My\n",
      "---------------\n",
      "\n",
      "Menenius, and grave we would that\n",
      "The celest of your presence.\n",
      "\n",
      "CORIOLANUS:\n",
      "What is the country?\n",
      "\n",
      "MENENIUS:\n",
      "Mark you, lords, is more than the sea\n",
      "In aught of the madness.\n",
      "\n",
      "CORIOLANUS:\n",
      "Mark of them, and you speak any better\n",
      "Than you shall loud to entreat the coronation. What\n",
      "I am wear for the view of my tongue\n",
      "And yet I had rather about his understranger,\n",
      "That thy fellows of the oracles of he submit\n",
      "Will have what thou wise silent worse kings as he\n",
      "Were a bad and choose, would have collen here\n",
      "To\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python -m microgpt.sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaibook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
