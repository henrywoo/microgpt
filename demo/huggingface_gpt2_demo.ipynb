{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73f5a1582a835ff",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df4711c598f4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_model():\n",
    "    \"\"\"Load and setup the GPT-2 model.\"\"\"\n",
    "    print(\"=== Loading Pre-trained GPT-2 Model ===\")\n",
    "    \n",
    "    # You can choose different model sizes:\n",
    "    # - 'gpt2' (124M parameters) - fastest\n",
    "    # - 'gpt2-medium' (355M parameters)\n",
    "    # - 'gpt2-large' (774M parameters)\n",
    "    # - 'gpt2-xl' (1.5B parameters) - slowest\n",
    "    \n",
    "    model_name = 'gpt2'  # Start with the smallest for faster loading\n",
    "    \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"âœ“ Model loaded: {model_name}\")\n",
    "    print(f\"âœ“ Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"âœ“ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    print(f\"âœ“ Model moved to: {device}\")\n",
    "    print()\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8690efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, device, prompt: str, max_length: int = 100, \n",
    "                  temperature: float = 1.0, top_k: int = 50, top_p: float = 0.9, \n",
    "                  do_sample: bool = True):\n",
    "    \"\"\"\n",
    "    Generate text from a given prompt using GPT-2.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_k: Top-k sampling parameter\n",
    "        top_p: Top-p (nucleus) sampling parameter\n",
    "        do_sample: Whether to use sampling or greedy decoding\n",
    "    \n",
    "    Returns:\n",
    "        Generated text and attention weights\n",
    "    \"\"\"\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text, outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30066c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def demo_text_generation(model, tokenizer, device):\n",
    "    \"\"\"Demonstrate text generation with different prompts and parameters.\"\"\"\n",
    "    print(\"=== Text Generation Examples ===\")\n",
    "    \n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"Once upon a time in a magical forest\",\n",
    "        \"The best way to learn programming is\",\n",
    "        \"In the year 2050, humanity will\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        \n",
    "        # Generate with different temperatures\n",
    "        for temp in [0.7, 1.0, 1.3]:\n",
    "            try:\n",
    "                generated_text, attentions = generate_text(\n",
    "                    model, tokenizer, device,\n",
    "                    prompt, \n",
    "                    max_length=len(prompt.split()) + 20,  # Add ~20 words\n",
    "                    temperature=temp,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "                print(f\"  Temperature {temp}: {generated_text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Temperature {temp}: Error - {e}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cdefcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention_weights(attentions: Tuple[torch.Tensor], \n",
    "                               tokens: List[str], \n",
    "                               layer_idx: int = 0, \n",
    "                               head_idx: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a specific layer and head.\n",
    "    \n",
    "    Args:\n",
    "        attentions: Tuple of attention tensors from model output\n",
    "        tokens: List of token strings\n",
    "        layer_idx: Index of the transformer layer to visualize\n",
    "        head_idx: Index of the attention head to visualize\n",
    "    \"\"\"\n",
    "    if not attentions:\n",
    "        print(\"No attention weights available\")\n",
    "        return\n",
    "    \n",
    "    # Get attention weights for the specified layer and head\n",
    "    attention = attentions[layer_idx][0, head_idx].cpu().numpy()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        attention, \n",
    "        xticklabels=tokens, \n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        annot=False,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Attention weight statistics for Layer {layer_idx}, Head {head_idx}:\")\n",
    "    print(f\"  Mean: {attention.mean():.4f}\")\n",
    "    print(f\"  Std: {attention.std():.4f}\")\n",
    "    print(f\"  Min: {attention.min():.4f}\")\n",
    "    print(f\"  Max: {attention.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb6252ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def demo_attention_visualization(model, tokenizer, device):\n",
    "    \"\"\"Demonstrate attention weight visualization.\"\"\"\n",
    "    print(\"=== Attention Weight Visualization ===\")\n",
    "    \n",
    "    # Generate text with attention tracking\n",
    "    prompt = \"The artificial intelligence revolution began\"\n",
    "    print(f\"Generating text from prompt: '{prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        generated_text, attentions = generate_text(\n",
    "            model, tokenizer, device,\n",
    "            prompt, \n",
    "            max_length=len(prompt.split()) + 15,\n",
    "            temperature=0.8,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "        print(f\"Number of layers: {len(attentions)}\")\n",
    "        print(f\"Attention shape per layer: {attentions[0].shape if attentions else 'N/A'}\")\n",
    "        \n",
    "        # Get tokens for visualization\n",
    "        tokens = tokenizer.tokenize(generated_text)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print()\n",
    "        \n",
    "        if attentions:\n",
    "            # Visualize first layer, first head\n",
    "            print(\"1. Single attention head visualization:\")\n",
    "            visualize_attention_weights(attentions, tokens, layer_idx=0, head_idx=0)\n",
    "            \n",
    "            # Visualize multiple heads from the same layer\n",
    "            print(\"\\n2. Multiple attention heads visualization:\")\n",
    "            \n",
    "            # Create subplot grid for multiple heads\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            axes = axes.ravel()\n",
    "            \n",
    "            for i in range(4):\n",
    "                attention = attentions[0][0, i].cpu().numpy()\n",
    "                \n",
    "                sns.heatmap(\n",
    "                    attention,\n",
    "                    xticklabels=tokens,\n",
    "                    yticklabels=tokens,\n",
    "                    cmap='Blues',\n",
    "                    annot=False,\n",
    "                    ax=axes[i],\n",
    "                    cbar_kws={'label': 'Attention Weight'}\n",
    "                )\n",
    "                \n",
    "                axes[i].set_title(f'Head {i}')\n",
    "                axes[i].set_xlabel('Key Tokens')\n",
    "                axes[i].set_ylabel('Query Tokens')\n",
    "                axes[i].tick_params(axis='x', rotation=45, ha='right')\n",
    "            \n",
    "            plt.suptitle('Attention Weights for Multiple Heads - Layer 0', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Visualize attention from different layers\n",
    "            print(\"\\n3. Attention from different layers:\")\n",
    "            for layer_idx in [0, len(attentions)//2, len(attentions)-1]:\n",
    "                print(f\"\\nLayer {layer_idx}:\")\n",
    "                visualize_attention_weights(attentions, tokens, layer_idx=layer_idx, head_idx=0)\n",
    "        else:\n",
    "            print(\"No attention weights available for visualization\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during attention visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_generation_strategies(model, tokenizer, device, prompt: str, max_length: int = 50):\n",
    "    \"\"\"Compare different text generation strategies.\"\"\"\n",
    "    print(f\"=== Generation Strategy Comparison ===\")\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    strategies = [\n",
    "        {\"name\": \"Greedy\", \"do_sample\": False, \"temperature\": 1.0},\n",
    "        {\"name\": \"Temperature 0.5\", \"do_sample\": True, \"temperature\": 0.5},\n",
    "        {\"name\": \"Temperature 1.0\", \"do_sample\": True, \"temperature\": 1.0},\n",
    "        {\"name\": \"Temperature 1.5\", \"do_sample\": True, \"temperature\": 1.5},\n",
    "        {\"name\": \"Top-k (k=10)\", \"do_sample\": True, \"temperature\": 1.0, \"top_k\": 10},\n",
    "        {\"name\": \"Top-p (p=0.9)\", \"do_sample\": True, \"temperature\": 1.0, \"top_p\": 0.9}\n",
    "    ]\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        try:\n",
    "            generated_text, _ = generate_text(\n",
    "                model, tokenizer, device,\n",
    "                prompt,\n",
    "                max_length=len(prompt.split()) + max_length,\n",
    "                **{k: v for k, v in strategy.items() if k != 'name'}\n",
    "            )\n",
    "            \n",
    "            print(f\"{strategy['name']}:\")\n",
    "            print(f\"  {generated_text}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{strategy['name']}: Error - {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7beba29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main demo function.\"\"\"\n",
    "    print(\"ðŸš€ HuggingFace Transformers GPT-2 Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check PyTorch and CUDA\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Setup model\n",
    "        model, tokenizer, device = setup_model()\n",
    "        \n",
    "        # Demo text generation\n",
    "        demo_text_generation(model, tokenizer, device)\n",
    "        \n",
    "        # Demo attention visualization\n",
    "        demo_attention_visualization(model, tokenizer, device)\n",
    "        \n",
    "        # Compare generation strategies\n",
    "        test_prompt = \"The future of technology lies in\"\n",
    "        compare_generation_strategies(model, tokenizer, device, test_prompt)\n",
    "        \n",
    "        print(\"=== Demo Completed Successfully! ===\")\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Try different model sizes (gpt2-medium, gpt2-large, gpt2-xl)\")\n",
    "        print(\"2. Experiment with different prompts and parameters\")\n",
    "        print(\"3. Explore attention patterns across different layers\")\n",
    "        print(\"4. Build your own text generation applications\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during demo: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30667585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ HuggingFace Transformers GPT-2 Demo\n",
      "==================================================\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4090\n",
      "\n",
      "=== Loading Pre-trained GPT-2 Model ===\n",
      "Loading gpt2...\n",
      "âœ“ Model loaded: gpt2\n",
      "âœ“ Vocabulary size: 50257\n",
      "âœ“ Model parameters: 124,439,808\n",
      "âœ“ Model moved to: cuda\n",
      "\n",
      "=== Text Generation Examples ===\n",
      "Prompt 1: The future of artificial intelligence is\n",
      "  Temperature 0.7: The future of artificial intelligence is unknown.\"\n",
      "\n",
      "\"It's really difficult to say,\" said CTO of Microsoft's AI division\n",
      "  Temperature 1.0: The future of artificial intelligence is in the hands of intelligent machines, and the future of humanity depends not on them but on us.\n",
      "  Temperature 1.3: The future of artificial intelligence is a debate on a high level of international scientific consensus on AI and robotics that are now getting out there\n",
      "\n",
      "Prompt 2: Once upon a time in a magical forest\n",
      "  Temperature 0.7: Once upon a time in a magical forest, a magical artifact that has been created by the magic of a particular goddess was created. The artifact\n",
      "  Temperature 1.0: Once upon a time in a magical forest the dragon was one that was in perfect control over its own body, and they were able to make\n",
      "  Temperature 1.3: Once upon a time in a magical forest, there were monsters, evil people, and a god! One would realize there was a large body\n",
      "\n",
      "Prompt 3: The best way to learn programming is\n",
      "  Temperature 0.7: The best way to learn programming is to learn programming. If you want to be a programmer, you have to learn programming. If you\n",
      "  Temperature 1.0: The best way to learn programming is to practice your programming skills and work with people who can handle your learning. If you're not familiar\n",
      "  Temperature 1.3: The best way to learn programming is to build it first with your basic knowledge.\" â€” Michael Duchovny, former CPA and\n",
      "\n",
      "Prompt 4: In the year 2050, humanity will\n",
      "  Temperature 0.7: In the year 2050, humanity will have reached its greatest capacity to solve the world's problems, and the planet is on track to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Temperature 1.0: In the year 2050, humanity will face a new crisis, driven by the rise of mass migration, a natural catastrophe that will impact\n",
      "  Temperature 1.3: In the year 2050, humanity will continue to lead the industrial revolution to an era where we will surpass human capacity for survival, the\n",
      "\n",
      "=== Attention Weight Visualization ===\n",
      "Generating text from prompt: 'The artificial intelligence revolution began'\n",
      "Generated text: The artificial intelligence revolution began with the launch of the AI-powered car.\n",
      "\n",
      "For decades,\n",
      "Number of layers: 15\n",
      "Error during attention visualization: 'tuple' object has no attribute 'shape'\n",
      "=== Generation Strategy Comparison ===\n",
      "Prompt: The future of technology lies in\n",
      "\n",
      "Greedy:\n",
      "  The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human\n",
      "\n",
      "Temperature 0.5:\n",
      "  The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human race.\n",
      "\n",
      "The future of technology lies in the future of the human\n",
      "\n",
      "Temperature 1.0:\n",
      "  The future of technology lies in the use of intelligent devices to enhance our lives and communities. The technology and devices that we now use to improve our daily lives make the world a much safer and more harmonious place to live.\n",
      "\n",
      "Our future will also include using smart products that\n",
      "\n",
      "Temperature 1.5:\n",
      "  The future of technology lies in the evolution of personal information security and privacy as companies work with the government to strengthen their own processes.\"\n",
      "\n",
      "\n",
      "There would, the National Telecommunications and Information Authority warned, face \"increasingly clear limits\" on data, including what is and is not personal\n",
      "\n",
      "Top-k (k=10):\n",
      "  The future of technology lies in the ability of people to do things they want, and to make decisions that are good for people.\n",
      "\n",
      "We need to do something about that. And I believe that we're at a point where technology is really making a huge difference in terms of\n",
      "\n",
      "Top-p (p=0.9):\n",
      "  The future of technology lies in the future. In today's world, we can go and do everything we want and do nothing at all. This is the most dangerous moment in humanity's history. That's the moment where we need to make sure our technology, and our freedom,\n",
      "\n",
      "=== Demo Completed Successfully! ===\n",
      "\n",
      "Next steps:\n",
      "1. Try different model sizes (gpt2-medium, gpt2-large, gpt2-xl)\n",
      "2. Experiment with different prompts and parameters\n",
      "3. Explore attention patterns across different layers\n",
      "4. Build your own text generation applications\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaibook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
